{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from utils import create_session_config\n",
    "import tensorflow_probability as tp\n",
    "import numpy as np\n",
    "from models.utils import covariance,correlation_matrix,tf_cov,assert_cov_positive_definite,corr,print_tensor,print_tensors,constrain_cdf\n",
    "from models.nn_pdf_common import transform_x,density_estimator,create_pdf_layer_mv,create_partially_monotone_dense_layer,\\\n",
    "    create_monotone_dense_layer,create_positive_weights,create_bias\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from flags import FLAGS\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import os\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "from data.registry import inv_sin\n",
    "from models.utils import get_train_inputs\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tp\n",
    "from models.utils import log_likelihood_from_cdfs_transforms, constrain_cdf, metric_loglikelihood, train_op, \\\n",
    "    print_tensor, add_all_summary_stats, extract_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data: inv_sin.npz\n"
     ]
    }
   ],
   "source": [
    "FLAGS.dir = '/home/pawel/PycharmProjects/RM_labs/tmp2/'\n",
    "os.chdir(os.path.join(FLAGS.dir,'inv_sin'))\n",
    "FLAGS.data_set=\"inv_sin\"\n",
    "data_loader = inv_sin() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mixtures(output, num_marginals):\n",
    "    marginal_params = tf.split(output, num_or_size_splits=num_marginals, axis=1)\n",
    "    mixtures = []\n",
    "\n",
    "    for marginal_id in range(num_marginals):\n",
    "        with tf.variable_scope(\"mixture_%d\"%marginal_id):\n",
    "            out_logits, out_sigma, out_mu = tf.split(marginal_params[marginal_id], num_or_size_splits=3, axis=1)\n",
    "            out_sigma = print_tensor(add_all_summary_stats(tf.maximum(1e-24,tf.square(out_sigma), name=\"sigma\")))\n",
    "\n",
    "            mix = tp.distributions.MixtureSameFamily(\n",
    "                mixture_distribution=tp.distributions.Categorical(logits=out_logits),\n",
    "                components_distribution=tp.distributions.Normal(loc=out_mu, scale=out_sigma))\n",
    "\n",
    "            mixtures.append(mix)\n",
    "    return mixtures\n",
    "\n",
    "def get_loss(log_likelihood):\n",
    "    return tf.negative(tf.reduce_mean(log_likelihood), name=\"loss\")\n",
    "\n",
    "def generate_ensemble(mix):\n",
    "    return tf.reshape(mix.sample(1, name=\"sample\"), [-1,1])\n",
    "\n",
    "def compute_log_prob_mixture(mix, y, i):\n",
    "    with tf.variable_scope(\"log_pdf_%d\"%i):\n",
    "        return tf.reshape(mix.log_prob(tf.reshape(y, [-1])),[-1,1])\n",
    "\n",
    "def log_likelihood(cov_type, mixtures, y, lls,x, params):\n",
    "    cdfs = []\n",
    "    for i, mix in enumerate(mixtures):\n",
    "        # this has to be 1-D so mixture computes parametrization per data point\n",
    "        y_component = tf.reshape(tf.slice(y, [0, i], size=[-1, 1]), [-1], name=\"y_component%d\"%i)\n",
    "        cdfs.append(constrain_cdf(tf.reshape(mix.cdf(y_component, name=\"component_%d_cdf\"%i),[-1,1])))\n",
    "\n",
    "    return log_likelihood_from_cdfs_transforms(cov_type, cdfs, lls, x, params)\n",
    "\n",
    "def compute_lls(mixtures, y):\n",
    "    return [compute_log_prob_mixture(mix, tf.slice(y, [0, i], size=[-1, 1]), i) for i, mix in enumerate(mixtures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step, loss:  [0, 13.745109]\n",
      "step: 1, loss: 14.099165916442871\n",
      "step: 1001, loss: 0.1770516186952591\n",
      "step: 2001, loss: 0.0430937260389328\n",
      "step: 3001, loss: 0.026956617832183838\n",
      "step: 4001, loss: -0.023929934948682785\n",
      "step: 5001, loss: -0.026690278202295303\n",
      "step: 6001, loss: -0.21177475154399872\n",
      "step: 7001, loss: -0.11148286610841751\n",
      "step: 8001, loss: -0.0903037041425705\n",
      "step: 9001, loss: -0.13646581768989563\n",
      "step: 10001, loss: -0.1312563121318817\n",
      "step: 11001, loss: -0.1992224156856537\n",
      "step: 12001, loss: -0.16778677701950073\n",
      "step: 13001, loss: -0.13733763992786407\n",
      "step: 14001, loss: -0.08363212645053864\n",
      "step: 15001, loss: -0.10829459875822067\n",
      "step: 16001, loss: -0.16952566802501678\n",
      "step: 17001, loss: -0.12203441560268402\n",
      "step: 18001, loss: -0.20326006412506104\n",
      "step: 19001, loss: -0.047660037875175476\n",
      "step: 20001, loss: -0.07615961134433746\n",
      "step: 21001, loss: -0.13758069276809692\n",
      "step: 22001, loss: -0.10781072825193405\n",
      "step: 23001, loss: -0.1789550930261612\n",
      "step: 24001, loss: -0.15321208536624908\n",
      "step: 25001, loss: -0.11093074083328247\n",
      "step: 26001, loss: -0.1978158950805664\n",
      "step: 27001, loss: -0.17905718088150024\n",
      "step: 28001, loss: -0.20066724717617035\n",
      "step: 29001, loss: -0.11466611176729202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method TF_Output.<lambda> of <tensorflow.python.pywrap_tensorflow_internal.TF_Output; proxy of <Swig Object of type 'TF_Output *' at 0x7f8d4c651fc0> >>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pawel/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 963, in <lambda>\n",
      "    __del__ = lambda self: None\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 30001, loss: -0.12085758149623871\n",
      "step: 31001, loss: -0.20843103528022766\n",
      "step: 32001, loss: -0.22926589846611023\n",
      "step: 33001, loss: -0.20736920833587646\n",
      "step: 34001, loss: -0.1003480926156044\n",
      "step: 35001, loss: -0.17286212742328644\n",
      "step: 36001, loss: -0.22291794419288635\n",
      "step: 37001, loss: -0.22853723168373108\n",
      "step: 38001, loss: -0.2658086121082306\n",
      "step: 39001, loss: -0.1630910038948059\n",
      "step: 40001, loss: -0.06404709815979004\n",
      "step: 41001, loss: -0.19753113389015198\n",
      "step: 42001, loss: -0.07987198233604431\n",
      "step: 43001, loss: -0.1788136512041092\n",
      "step: 44001, loss: -0.3183062672615051\n",
      "step: 45001, loss: -0.15315082669258118\n",
      "step: 46001, loss: -0.3259912133216858\n",
      "step: 47001, loss: -0.15781714022159576\n",
      "step: 48001, loss: -0.23492465913295746\n",
      "step: 49001, loss: -0.22411257028579712\n",
      "step: 50001, loss: -0.115387462079525\n",
      "step: 51001, loss: -0.15677422285079956\n",
      "step: 52001, loss: -0.18423543870449066\n",
      "step: 53001, loss: -0.2031368613243103\n",
      "step: 54001, loss: -0.2183283269405365\n",
      "step: 55001, loss: -0.26241356134414673\n",
      "step: 56001, loss: -0.15414567291736603\n",
      "step: 57001, loss: -0.20405474305152893\n",
      "step: 58001, loss: -0.19130292534828186\n",
      "step: 59001, loss: -0.1949080526828766\n",
      "step: 60001, loss: -0.16312101483345032\n",
      "step: 61001, loss: -0.2383430302143097\n",
      "step: 62001, loss: -0.28887349367141724\n",
      "step: 63001, loss: -0.3131331503391266\n",
      "step: 64001, loss: -0.16850602626800537\n",
      "step: 65001, loss: -0.2168189436197281\n",
      "step: 66001, loss: -0.25600600242614746\n",
      "step: 67001, loss: -0.2402794063091278\n",
      "step: 68001, loss: -0.24112223088741302\n",
      "step: 69001, loss: -0.23302911221981049\n",
      "step: 70001, loss: -0.24465765058994293\n",
      "step: 71001, loss: -0.17682383954524994\n",
      "step: 72001, loss: -0.2292395383119583\n",
      "step: 73001, loss: -0.30422255396842957\n",
      "step: 74001, loss: -0.2156398594379425\n",
      "step: 75001, loss: -0.23980161547660828\n",
      "step: 76001, loss: -0.18462955951690674\n",
      "step: 77001, loss: -0.19403667747974396\n",
      "step: 78001, loss: -0.2207917720079422\n",
      "step: 79001, loss: -0.16672895848751068\n",
      "step: 80001, loss: -0.2519606649875641\n",
      "step: 81001, loss: -0.2742539644241333\n",
      "step: 82001, loss: -0.26355689764022827\n",
      "step: 83001, loss: -0.14144398272037506\n",
      "step: 84001, loss: -0.19857139885425568\n",
      "step: 85001, loss: -0.27228569984436035\n",
      "step: 86001, loss: -0.21851180493831635\n",
      "step: 87001, loss: -0.15938794612884521\n",
      "step: 88001, loss: -0.2143268585205078\n",
      "step: 89001, loss: -0.21377813816070557\n",
      "step: 90001, loss: -0.26739153265953064\n",
      "step: 91001, loss: -0.13238537311553955\n",
      "step: 92001, loss: -0.24586816132068634\n",
      "step: 93001, loss: -0.22893080115318298\n",
      "step: 94001, loss: -0.25859495997428894\n",
      "step: 95001, loss: -0.2546004354953766\n",
      "step: 96001, loss: -0.26776939630508423\n",
      "step: 97001, loss: -0.3295050859451294\n",
      "step: 98001, loss: -0.18257351219654083\n",
      "step: 99001, loss: -0.23968864977359772\n",
      "step: 100001, loss: -0.14586609601974487\n",
      "step: 101001, loss: -0.34797367453575134\n",
      "step: 102001, loss: -0.3496401607990265\n",
      "step: 103001, loss: -0.16242431104183197\n",
      "step: 104001, loss: -0.1876824051141739\n",
      "step: 105001, loss: -0.310886025428772\n",
      "step: 106001, loss: -0.1441500037908554\n",
      "step: 107001, loss: -0.22514601051807404\n",
      "step: 108001, loss: -0.09416163712739944\n",
      "step: 109001, loss: -0.2736014425754547\n",
      "step: 110001, loss: -0.21415139734745026\n",
      "step: 111001, loss: -0.23450851440429688\n",
      "step: 112001, loss: -0.1860164850950241\n",
      "step: 113001, loss: -0.27709880471229553\n",
      "step: 114001, loss: -0.21052776277065277\n",
      "step: 115001, loss: -0.31624457240104675\n",
      "step: 116001, loss: -0.19498798251152039\n",
      "step: 117001, loss: -0.28522905707359314\n",
      "step: 118001, loss: -0.16133861243724823\n",
      "step: 119001, loss: -0.22484420239925385\n",
      "step: 120001, loss: -0.30436018109321594\n",
      "step: 121001, loss: -0.25381600856781006\n",
      "step: 122001, loss: -0.262386679649353\n",
      "step: 123001, loss: -0.32628628611564636\n",
      "step: 124001, loss: -0.1605483591556549\n",
      "step: 125001, loss: -0.2699020504951477\n",
      "step: 126001, loss: -0.3022734820842743\n",
      "step: 127001, loss: -0.16431313753128052\n",
      "step: 128001, loss: -0.14532315731048584\n",
      "step: 129001, loss: -0.18786142766475677\n",
      "step: 130001, loss: -0.2176668494939804\n",
      "step: 131001, loss: -0.33217495679855347\n",
      "step: 132001, loss: -0.2848406136035919\n",
      "step: 133001, loss: -0.23298510909080505\n",
      "step: 134001, loss: -0.21907810866832733\n",
      "step: 135001, loss: -0.23094138503074646\n",
      "step: 136001, loss: -0.3047550916671753\n",
      "step: 137001, loss: -0.2683965265750885\n",
      "step: 138001, loss: -0.15808185935020447\n",
      "step: 139001, loss: -0.2809949517250061\n",
      "step: 140001, loss: -0.1764403134584427\n",
      "step: 141001, loss: -0.2507759928703308\n",
      "step: 142001, loss: -0.18639139831066132\n",
      "step: 143001, loss: -0.15886782109737396\n",
      "step: 144001, loss: -0.2008506953716278\n",
      "step: 145001, loss: -0.15859566628932953\n",
      "step: 146001, loss: -0.24798113107681274\n",
      "step: 147001, loss: -0.209228515625\n",
      "step: 148001, loss: -0.1642875224351883\n",
      "step: 149001, loss: -0.25989407300949097\n",
      "step: 150001, loss: -0.31945425271987915\n",
      "step: 151001, loss: -0.2795886695384979\n",
      "step: 152001, loss: -0.19550414383411407\n",
      "step: 153001, loss: -0.2203933596611023\n",
      "step: 154001, loss: -0.25088948011398315\n",
      "step: 155001, loss: -0.2506190240383148\n",
      "step: 156001, loss: -0.20882722735404968\n",
      "step: 157001, loss: -0.1716436743736267\n",
      "step: 158001, loss: -0.27467313408851624\n",
      "step: 159001, loss: -0.22932127118110657\n",
      "step: 160001, loss: -0.13346225023269653\n",
      "step: 161001, loss: -0.29017654061317444\n",
      "step: 162001, loss: -0.2595396339893341\n",
      "step: 163001, loss: -0.31534290313720703\n",
      "step: 164001, loss: -0.31790050864219666\n",
      "step: 165001, loss: -0.2362412065267563\n",
      "step: 166001, loss: -0.32253846526145935\n",
      "step: 167001, loss: -0.21324200928211212\n",
      "step: 168001, loss: -0.17873238027095795\n",
      "step: 169001, loss: -0.3090001940727234\n",
      "step: 170001, loss: -0.3851989507675171\n",
      "step: 171001, loss: -0.2497491091489792\n",
      "step: 172001, loss: -0.30627143383026123\n",
      "step: 173001, loss: -0.280161589384079\n",
      "step: 174001, loss: -0.18550169467926025\n",
      "step: 175001, loss: -0.23399142920970917\n",
      "step: 176001, loss: -0.23030562698841095\n",
      "step: 177001, loss: -0.40816643834114075\n",
      "step: 178001, loss: -0.22218722105026245\n",
      "step: 179001, loss: -0.18017929792404175\n",
      "step: 180001, loss: -0.219485342502594\n",
      "step: 181001, loss: -0.29697102308273315\n",
      "step: 182001, loss: -0.27120867371559143\n",
      "step: 183001, loss: -0.22278091311454773\n",
      "step: 184001, loss: -0.23760132491588593\n",
      "step: 185001, loss: -0.28546714782714844\n",
      "step: 186001, loss: -0.241910919547081\n",
      "step: 187001, loss: -0.2817463278770447\n",
      "step: 188001, loss: -0.4020649790763855\n",
      "step: 189001, loss: -0.24089470505714417\n",
      "step: 190001, loss: -0.2999565005302429\n",
      "step: 191001, loss: -0.14268726110458374\n",
      "step: 192001, loss: -0.13963375985622406\n",
      "step: 193001, loss: -0.24198077619075775\n",
      "step: 194001, loss: -0.3300488591194153\n",
      "step: 195001, loss: -0.21118536591529846\n",
      "step: 196001, loss: -0.20627044141292572\n",
      "step: 197001, loss: -0.1446959376335144\n",
      "step: 198001, loss: -0.2791992425918579\n",
      "step: 199001, loss: -0.2749614119529724\n",
      "step: 200001, loss: -0.25301745533943176\n",
      "step: 201001, loss: -0.20038895308971405\n",
      "step: 202001, loss: -0.16416536271572113\n",
      "step: 203001, loss: -0.2817939519882202\n",
      "step: 204001, loss: -0.2133234739303589\n",
      "step: 205001, loss: -0.3166864514350891\n",
      "step: 206001, loss: -0.24452786147594452\n",
      "step: 207001, loss: -0.22764988243579865\n",
      "step: 208001, loss: -0.34412968158721924\n",
      "step: 209001, loss: -0.26102179288864136\n",
      "step: 210001, loss: -0.274495393037796\n",
      "step: 211001, loss: -0.25461018085479736\n",
      "step: 212001, loss: -0.26608458161354065\n",
      "step: 213001, loss: -0.27789628505706787\n",
      "step: 214001, loss: -0.167909637093544\n",
      "step: 215001, loss: -0.11018078029155731\n",
      "step: 216001, loss: -0.171720489859581\n",
      "step: 217001, loss: -0.2775920033454895\n",
      "step: 218001, loss: -0.30040889978408813\n",
      "step: 219001, loss: -0.3074800670146942\n",
      "step: 220001, loss: -0.3541848361492157\n",
      "step: 221001, loss: -0.2266758531332016\n",
      "step: 222001, loss: -0.28570324182510376\n",
      "step: 223001, loss: -0.22381314635276794\n",
      "step: 224001, loss: -0.3018333911895752\n",
      "step: 225001, loss: -0.3580390512943268\n",
      "step: 226001, loss: -0.3384338617324829\n",
      "step: 227001, loss: -0.33298370242118835\n",
      "step: 228001, loss: -0.3704444169998169\n",
      "step: 229001, loss: -0.35073092579841614\n",
      "step: 230001, loss: -0.22402839362621307\n",
      "step: 231001, loss: -0.2439045011997223\n",
      "step: 232001, loss: -0.27397847175598145\n",
      "step: 233001, loss: -0.2938341200351715\n",
      "step: 234001, loss: -0.22543540596961975\n",
      "step: 235001, loss: -0.33150848746299744\n",
      "step: 236001, loss: -0.28405529260635376\n",
      "step: 237001, loss: -0.24317947030067444\n",
      "step: 238001, loss: -0.1641014814376831\n",
      "step: 239001, loss: -0.2929232716560364\n",
      "step: 240001, loss: -0.21340106427669525\n",
      "step: 241001, loss: -0.2873961329460144\n",
      "step: 242001, loss: -0.16171182692050934\n",
      "step: 243001, loss: -0.3704683184623718\n",
      "step: 244001, loss: -0.22160647809505463\n",
      "step: 245001, loss: -0.39386940002441406\n",
      "step: 246001, loss: -0.3632461130619049\n",
      "step: 247001, loss: -0.21256788074970245\n",
      "step: 248001, loss: -0.22291240096092224\n",
      "step: 249001, loss: -0.24462613463401794\n",
      "step: 250001, loss: -0.275600790977478\n",
      "step: 251001, loss: -0.24530667066574097\n",
      "step: 252001, loss: -0.18060968816280365\n",
      "step: 253001, loss: -0.30550530552864075\n",
      "step: 254001, loss: -0.19854940474033356\n",
      "step: 255001, loss: -0.23679302632808685\n",
      "step: 256001, loss: -0.26155027747154236\n",
      "step: 257001, loss: -0.2834536135196686\n",
      "step: 258001, loss: -0.31578558683395386\n",
      "step: 259001, loss: -0.22682829201221466\n",
      "step: 260001, loss: -0.32585403323173523\n",
      "step: 261001, loss: -0.27827340364456177\n",
      "step: 262001, loss: -0.3242169916629791\n",
      "step: 263001, loss: -0.16686199605464935\n",
      "step: 264001, loss: -0.37622305750846863\n",
      "step: 265001, loss: -0.25668638944625854\n",
      "step: 266001, loss: -0.4040093719959259\n",
      "step: 267001, loss: -0.24021807312965393\n",
      "step: 268001, loss: -0.3077738285064697\n",
      "step: 269001, loss: -0.28653353452682495\n",
      "step: 270001, loss: -0.25378328561782837\n",
      "step: 271001, loss: -0.3687666356563568\n",
      "step: 272001, loss: -0.3336730897426605\n",
      "step: 273001, loss: -0.26632922887802124\n",
      "step: 274001, loss: -0.29081690311431885\n",
      "step: 275001, loss: -0.32645493745803833\n",
      "step: 276001, loss: -0.305637001991272\n",
      "step: 277001, loss: -0.18821492791175842\n",
      "step: 278001, loss: -0.2656781077384949\n",
      "step: 279001, loss: -0.2812840938568115\n",
      "step: 280001, loss: -0.32727906107902527\n",
      "step: 281001, loss: -0.31355372071266174\n",
      "step: 282001, loss: -0.3216707110404968\n",
      "step: 283001, loss: -0.2702261209487915\n",
      "step: 284001, loss: -0.2997897267341614\n",
      "step: 285001, loss: -0.1486205905675888\n",
      "step: 286001, loss: -0.1757449358701706\n",
      "step: 287001, loss: -0.21451781690120697\n",
      "step: 288001, loss: -0.16616427898406982\n",
      "step: 289001, loss: -0.28561487793922424\n",
      "step: 290001, loss: -0.25414538383483887\n",
      "step: 291001, loss: -0.19508670270442963\n",
      "step: 292001, loss: -0.33682096004486084\n",
      "step: 293001, loss: -0.33858487010002136\n",
      "step: 294001, loss: -0.23848013579845428\n",
      "step: 295001, loss: -0.24983778595924377\n",
      "step: 296001, loss: -0.2631431818008423\n",
      "step: 297001, loss: -0.2819541096687317\n",
      "step: 298001, loss: -0.28679850697517395\n",
      "step: 299001, loss: -0.31491291522979736\n",
      "step: 300001, loss: -0.30372151732444763\n",
      "step: 301001, loss: -0.27250218391418457\n",
      "step: 302001, loss: -0.2547335922718048\n",
      "step: 303001, loss: -0.3361906409263611\n",
      "step: 304001, loss: -0.19362184405326843\n",
      "step: 305001, loss: -0.2022886872291565\n",
      "step: 306001, loss: -0.32151320576667786\n",
      "step: 307001, loss: -0.25505053997039795\n",
      "step: 308001, loss: -0.3013564944267273\n",
      "step: 309001, loss: -0.27484679222106934\n",
      "step: 310001, loss: -0.26624631881713867\n",
      "step: 311001, loss: -0.2451944351196289\n",
      "step: 312001, loss: -0.20834818482398987\n",
      "step: 313001, loss: -0.33029869198799133\n",
      "step: 314001, loss: -0.3497494161128998\n",
      "step: 315001, loss: -0.2873816192150116\n",
      "step: 316001, loss: -0.17380456626415253\n",
      "step: 317001, loss: -0.3332604467868805\n",
      "step: 318001, loss: -0.28663912415504456\n",
      "step: 319001, loss: -0.2202938050031662\n",
      "step: 320001, loss: -0.27962708473205566\n",
      "step: 321001, loss: -0.26419568061828613\n",
      "step: 322001, loss: -0.1747227907180786\n",
      "step: 323001, loss: -0.20354346930980682\n",
      "step: 324001, loss: -0.3593907952308655\n",
      "step: 325001, loss: -0.31732040643692017\n",
      "step: 326001, loss: -0.12748214602470398\n",
      "step: 327001, loss: -0.25041571259498596\n",
      "step: 328001, loss: -0.2923462390899658\n",
      "step: 329001, loss: -0.24295485019683838\n",
      "step: 330001, loss: -0.20763404667377472\n",
      "step: 331001, loss: -0.32359063625335693\n",
      "step: 332001, loss: -0.28568482398986816\n",
      "step: 333001, loss: -0.29053303599357605\n",
      "step: 334001, loss: -0.3662450313568115\n",
      "step: 335001, loss: -0.21500752866268158\n",
      "step: 336001, loss: -0.21773546934127808\n",
      "step: 337001, loss: -0.29417914152145386\n",
      "step: 338001, loss: -0.38195690512657166\n",
      "step: 339001, loss: -0.22748836874961853\n",
      "step: 340001, loss: -0.24742498993873596\n",
      "step: 341001, loss: -0.370705783367157\n",
      "step: 342001, loss: -0.2908940017223358\n",
      "step: 343001, loss: -0.2184537947177887\n",
      "step: 344001, loss: -0.2065984159708023\n",
      "step: 345001, loss: -0.37752220034599304\n",
      "step: 346001, loss: -0.3063686490058899\n",
      "step: 347001, loss: -0.23559774458408356\n",
      "step: 348001, loss: -0.23818227648735046\n",
      "step: 349001, loss: -0.2957364618778229\n",
      "step: 350001, loss: -0.2574099600315094\n",
      "step: 351001, loss: -0.23575685918331146\n",
      "step: 352001, loss: -0.2973317801952362\n",
      "step: 353001, loss: -0.21616323292255402\n",
      "step: 354001, loss: -0.27414876222610474\n",
      "step: 355001, loss: -0.29869139194488525\n",
      "step: 356001, loss: -0.22564221918582916\n",
      "step: 357001, loss: -0.2732612192630768\n",
      "step: 358001, loss: -0.2508623003959656\n",
      "step: 359001, loss: -0.31042468547821045\n",
      "step: 360001, loss: -0.216954305768013\n",
      "step: 361001, loss: -0.26313522458076477\n",
      "step: 362001, loss: -0.22199594974517822\n",
      "step: 363001, loss: -0.18153716623783112\n",
      "step: 364001, loss: -0.20392721891403198\n",
      "step: 365001, loss: -0.37849730253219604\n",
      "step: 366001, loss: -0.3914050757884979\n",
      "step: 367001, loss: -0.262989342212677\n",
      "step: 368001, loss: -0.19202843308448792\n",
      "step: 369001, loss: -0.3330714702606201\n",
      "step: 370001, loss: -0.15338344871997833\n",
      "step: 371001, loss: -0.2858891189098358\n",
      "step: 372001, loss: -0.2189946323633194\n",
      "step: 373001, loss: -0.29969704151153564\n",
      "step: 374001, loss: -0.3038201630115509\n",
      "step: 375001, loss: -0.30554845929145813\n",
      "step: 376001, loss: -0.19372639060020447\n",
      "step: 377001, loss: -0.4080228805541992\n",
      "step: 378001, loss: -0.18042702972888947\n",
      "step: 379001, loss: -0.30730050802230835\n",
      "step: 380001, loss: -0.28792017698287964\n",
      "step: 381001, loss: -0.28492188453674316\n",
      "step: 382001, loss: -0.30134814977645874\n",
      "step: 383001, loss: -0.25281113386154175\n",
      "step: 384001, loss: -0.25376081466674805\n",
      "step: 385001, loss: -0.27714261412620544\n",
      "step: 386001, loss: -0.30098721385002136\n",
      "step: 387001, loss: -0.30190372467041016\n",
      "step: 388001, loss: -0.3221728503704071\n",
      "step: 389001, loss: -0.24432463943958282\n",
      "step: 390001, loss: -0.2903134226799011\n",
      "step: 391001, loss: -0.27922293543815613\n",
      "step: 392001, loss: -0.3410570025444031\n",
      "step: 393001, loss: -0.30331650376319885\n",
      "step: 394001, loss: -0.2899661362171173\n",
      "step: 395001, loss: -0.33161985874176025\n",
      "step: 396001, loss: -0.3307822346687317\n",
      "step: 397001, loss: -0.23482103645801544\n",
      "step: 398001, loss: -0.12170597165822983\n",
      "step: 399001, loss: -0.22273510694503784\n",
      "step: 400001, loss: -0.3064222037792206\n",
      "step: 401001, loss: -0.2748569846153259\n",
      "step: 402001, loss: -0.33582472801208496\n",
      "step: 403001, loss: -0.32062241435050964\n",
      "step: 404001, loss: -0.3467273414134979\n",
      "step: 405001, loss: -0.35810813307762146\n",
      "step: 406001, loss: -0.3126913607120514\n",
      "step: 407001, loss: -0.22032347321510315\n",
      "step: 408001, loss: -0.15987412631511688\n",
      "step: 409001, loss: -0.3037252724170685\n",
      "step: 410001, loss: -0.21314090490341187\n",
      "step: 411001, loss: -0.20061708986759186\n",
      "step: 412001, loss: -0.2106737345457077\n",
      "step: 413001, loss: -0.2833825945854187\n",
      "step: 414001, loss: -0.19625042378902435\n",
      "step: 415001, loss: -0.16992774605751038\n",
      "step: 416001, loss: -0.23737919330596924\n",
      "step: 417001, loss: -0.3479134440422058\n",
      "step: 418001, loss: -0.28538215160369873\n",
      "step: 419001, loss: -0.19728000462055206\n",
      "step: 420001, loss: -0.3075225353240967\n",
      "step: 421001, loss: -0.27652695775032043\n",
      "step: 422001, loss: -0.340839147567749\n",
      "step: 423001, loss: -0.30671197175979614\n",
      "step: 424001, loss: -0.3170233368873596\n",
      "step: 425001, loss: -0.3092944920063019\n",
      "step: 426001, loss: -0.1978248804807663\n",
      "step: 427001, loss: -0.26067885756492615\n",
      "step: 428001, loss: -0.2572331130504608\n",
      "step: 429001, loss: -0.298465758562088\n",
      "step: 430001, loss: -0.24270512163639069\n",
      "step: 431001, loss: -0.25843918323516846\n",
      "step: 432001, loss: -0.31629177927970886\n",
      "step: 433001, loss: -0.38731738924980164\n",
      "step: 434001, loss: -0.34104374051094055\n",
      "step: 435001, loss: -0.220055490732193\n",
      "step: 436001, loss: -0.3127163052558899\n",
      "step: 437001, loss: -0.18012970685958862\n",
      "step: 438001, loss: -0.26736876368522644\n",
      "step: 439001, loss: -0.33491912484169006\n",
      "step: 440001, loss: -0.23514237999916077\n",
      "step: 441001, loss: -0.29344841837882996\n",
      "step: 442001, loss: -0.2961966395378113\n",
      "step: 443001, loss: -0.18943512439727783\n",
      "step: 444001, loss: -0.26189950108528137\n",
      "step: 445001, loss: -0.23881955444812775\n",
      "step: 446001, loss: -0.3159372806549072\n",
      "step: 447001, loss: -0.17772313952445984\n",
      "step: 448001, loss: -0.25287961959838867\n",
      "step: 449001, loss: -0.28677600622177124\n",
      "step: 450001, loss: -0.22893373668193817\n",
      "step: 451001, loss: -0.21768875420093536\n",
      "step: 452001, loss: -0.32486245036125183\n",
      "step: 453001, loss: -0.3058166205883026\n",
      "step: 454001, loss: -0.23142684996128082\n",
      "step: 455001, loss: -0.2734129726886749\n",
      "step: 456001, loss: -0.26492446660995483\n",
      "step: 457001, loss: -0.31775009632110596\n",
      "step: 458001, loss: -0.3302423059940338\n",
      "step: 459001, loss: -0.21235696971416473\n",
      "step: 460001, loss: -0.2221071422100067\n",
      "step: 461001, loss: -0.18132232129573822\n",
      "step: 462001, loss: -0.29814642667770386\n",
      "step: 463001, loss: -0.31108441948890686\n",
      "step: 464001, loss: -0.1886603981256485\n",
      "step: 465001, loss: -0.3181722164154053\n",
      "step: 466001, loss: -0.27236220240592957\n",
      "step: 467001, loss: -0.3353472054004669\n",
      "step: 468001, loss: -0.28018832206726074\n",
      "step: 469001, loss: -0.39262154698371887\n",
      "step: 470001, loss: -0.30330950021743774\n",
      "step: 471001, loss: -0.20412883162498474\n",
      "step: 472001, loss: -0.269522488117218\n",
      "step: 473001, loss: -0.30557528138160706\n",
      "step: 474001, loss: -0.294978529214859\n",
      "step: 475001, loss: -0.36642688512802124\n",
      "step: 476001, loss: -0.23208574950695038\n",
      "step: 477001, loss: -0.28730401396751404\n",
      "step: 478001, loss: -0.40273526310920715\n",
      "step: 479001, loss: -0.26171326637268066\n",
      "step: 480001, loss: -0.23238040506839752\n",
      "step: 481001, loss: -0.35569828748703003\n",
      "step: 482001, loss: -0.3354276716709137\n",
      "step: 483001, loss: -0.22077129781246185\n",
      "step: 484001, loss: -0.30782201886177063\n",
      "step: 485001, loss: -0.34329384565353394\n",
      "step: 486001, loss: -0.34757182002067566\n",
      "step: 487001, loss: -0.25887638330459595\n",
      "step: 488001, loss: -0.21849437057971954\n",
      "step: 489001, loss: -0.2751816511154175\n",
      "step: 490001, loss: -0.261406809091568\n",
      "step: 491001, loss: -0.38720718026161194\n",
      "step: 492001, loss: -0.25341200828552246\n",
      "step: 493001, loss: -0.2919074296951294\n",
      "step: 494001, loss: -0.3381042182445526\n",
      "step: 495001, loss: -0.2701471447944641\n",
      "step: 496001, loss: -0.29511910676956177\n",
      "step: 497001, loss: -0.3040737211704254\n",
      "step: 498001, loss: -0.333210825920105\n",
      "step: 499001, loss: -0.3649398684501648\n",
      "step: 500001, loss: -0.26755863428115845\n",
      "step: 501001, loss: -0.28987908363342285\n",
      "step: 502001, loss: -0.2894953489303589\n",
      "step: 503001, loss: -0.34436488151550293\n",
      "step: 504001, loss: -0.2412034273147583\n",
      "step: 505001, loss: -0.1121222972869873\n",
      "step: 506001, loss: -0.2891213297843933\n",
      "step: 507001, loss: -0.40176689624786377\n",
      "step: 508001, loss: -0.23564378917217255\n",
      "step: 509001, loss: -0.26622018218040466\n",
      "step: 510001, loss: -0.35123249888420105\n",
      "step: 511001, loss: -0.26898428797721863\n",
      "step: 512001, loss: -0.22903551161289215\n",
      "step: 513001, loss: -0.34473320841789246\n",
      "step: 514001, loss: -0.2720390260219574\n",
      "step: 515001, loss: -0.1934630572795868\n",
      "step: 516001, loss: -0.181763157248497\n",
      "step: 517001, loss: -0.16883552074432373\n",
      "step: 518001, loss: -0.22424161434173584\n",
      "step: 519001, loss: -0.2666045129299164\n",
      "step: 520001, loss: -0.2823668122291565\n",
      "step: 521001, loss: -0.36419638991355896\n",
      "step: 522001, loss: -0.18475458025932312\n",
      "step: 523001, loss: -0.16517843306064606\n",
      "step: 524001, loss: -0.30424195528030396\n",
      "step: 525001, loss: -0.2889350354671478\n",
      "step: 526001, loss: -0.1993175894021988\n",
      "step: 527001, loss: -0.35591939091682434\n",
      "step: 528001, loss: -0.27083003520965576\n",
      "step: 529001, loss: -0.35740864276885986\n",
      "step: 530001, loss: -0.3942200541496277\n",
      "step: 531001, loss: -0.32678085565567017\n",
      "step: 532001, loss: -0.3498477041721344\n",
      "step: 533001, loss: -0.39702704548835754\n",
      "step: 534001, loss: -0.3142624795436859\n",
      "step: 535001, loss: -0.23532095551490784\n",
      "step: 536001, loss: -0.3233284652233124\n",
      "step: 537001, loss: -0.2874314785003662\n",
      "step: 538001, loss: -0.25709235668182373\n",
      "step: 539001, loss: -0.23932431638240814\n",
      "step: 540001, loss: -0.25666627287864685\n",
      "step: 541001, loss: -0.3182445764541626\n",
      "step: 542001, loss: -0.34847378730773926\n",
      "step: 543001, loss: -0.24934978783130646\n",
      "step: 544001, loss: -0.27658364176750183\n",
      "step: 545001, loss: -0.3807060122489929\n",
      "step: 546001, loss: -0.2443528175354004\n",
      "step: 547001, loss: -0.22025758028030396\n",
      "step: 548001, loss: -0.24611911177635193\n",
      "step: 549001, loss: -0.26121318340301514\n",
      "step: 550001, loss: -0.24020548164844513\n",
      "step: 551001, loss: -0.17541399598121643\n",
      "step: 552001, loss: -0.1920282393693924\n",
      "step: 553001, loss: -0.2656785249710083\n",
      "step: 554001, loss: -0.3560856282711029\n",
      "step: 555001, loss: -0.3638380765914917\n",
      "step: 556001, loss: -0.1888866275548935\n",
      "step: 557001, loss: -0.32176029682159424\n",
      "step: 558001, loss: -0.07832157611846924\n",
      "step: 559001, loss: -0.17621742188930511\n",
      "step: 560001, loss: -0.22672230005264282\n",
      "step: 561001, loss: -0.34618788957595825\n",
      "step: 562001, loss: -0.273031622171402\n",
      "step: 563001, loss: -0.31704819202423096\n",
      "step: 564001, loss: -0.2654651403427124\n",
      "step: 565001, loss: -0.39743736386299133\n",
      "step: 566001, loss: -0.38240504264831543\n",
      "step: 567001, loss: -0.33435574173927307\n",
      "step: 568001, loss: -0.3522337079048157\n",
      "step: 569001, loss: -0.3684106469154358\n",
      "step: 570001, loss: -0.3016422688961029\n",
      "step: 571001, loss: -0.3431621789932251\n",
      "step: 572001, loss: -0.30082449316978455\n",
      "step: 573001, loss: -0.26444128155708313\n",
      "step: 574001, loss: -0.2850542664527893\n",
      "step: 575001, loss: -0.20535564422607422\n",
      "step: 576001, loss: -0.2700573205947876\n",
      "step: 577001, loss: -0.30256330966949463\n",
      "step: 578001, loss: -0.23747128248214722\n",
      "step: 579001, loss: -0.35140150785446167\n",
      "step: 580001, loss: -0.25117674469947815\n",
      "step: 581001, loss: -0.37148037552833557\n",
      "step: 582001, loss: -0.31389185786247253\n",
      "step: 583001, loss: -0.3427242040634155\n",
      "step: 584001, loss: -0.32724303007125854\n",
      "step: 585001, loss: -0.3361518383026123\n",
      "step: 586001, loss: -0.26819807291030884\n",
      "step: 587001, loss: -0.36488083004951477\n",
      "step: 588001, loss: -0.33021509647369385\n",
      "step: 589001, loss: -0.2584711015224457\n",
      "step: 590001, loss: -0.36040419340133667\n",
      "step: 591001, loss: -0.20008167624473572\n",
      "step: 592001, loss: -0.17084287106990814\n",
      "step: 593001, loss: -0.2719019055366516\n",
      "step: 594001, loss: -0.25534147024154663\n",
      "step: 595001, loss: -0.3934740126132965\n",
      "step: 596001, loss: -0.3122076988220215\n",
      "step: 597001, loss: -0.41928568482398987\n",
      "step: 598001, loss: -0.1481471210718155\n",
      "step: 599001, loss: -0.31103724241256714\n",
      "step: 600001, loss: -0.34686097502708435\n",
      "step: 601001, loss: -0.28951385617256165\n",
      "step: 602001, loss: -0.18532773852348328\n",
      "step: 603001, loss: -0.2293340563774109\n",
      "step: 604001, loss: -0.28839564323425293\n",
      "step: 605001, loss: -0.37896645069122314\n",
      "step: 606001, loss: -0.2712026834487915\n",
      "step: 607001, loss: -0.3318575322628021\n",
      "step: 608001, loss: -0.32926440238952637\n",
      "step: 609001, loss: -0.2426770031452179\n",
      "step: 610001, loss: -0.2749461829662323\n",
      "step: 611001, loss: -0.3349968194961548\n",
      "step: 612001, loss: -0.21307817101478577\n",
      "step: 613001, loss: -0.377832293510437\n",
      "step: 614001, loss: -0.28356999158859253\n",
      "step: 615001, loss: -0.23838092386722565\n",
      "step: 616001, loss: -0.26939499378204346\n",
      "step: 617001, loss: -0.42459383606910706\n",
      "step: 618001, loss: -0.2893945276737213\n",
      "step: 619001, loss: -0.23588907718658447\n",
      "step: 620001, loss: -0.24243421852588654\n",
      "step: 621001, loss: -0.376776784658432\n",
      "step: 622001, loss: -0.2941395044326782\n",
      "step: 623001, loss: -0.1489037573337555\n",
      "step: 624001, loss: -0.39103975892066956\n",
      "step: 625001, loss: -0.3456367552280426\n",
      "step: 626001, loss: -0.26814642548561096\n",
      "step: 627001, loss: -0.29696452617645264\n",
      "step: 628001, loss: -0.2513740658760071\n",
      "step: 629001, loss: -0.26876455545425415\n",
      "step: 630001, loss: -0.3361610174179077\n",
      "step: 631001, loss: -0.24495011568069458\n",
      "step: 632001, loss: -0.2368963211774826\n",
      "step: 633001, loss: -0.23293915390968323\n",
      "step: 634001, loss: -0.28771984577178955\n",
      "step: 635001, loss: -0.3272617757320404\n",
      "step: 636001, loss: -0.3101009726524353\n",
      "step: 637001, loss: -0.2445024847984314\n",
      "step: 638001, loss: -0.34466856718063354\n",
      "step: 639001, loss: -0.12708143889904022\n",
      "step: 640001, loss: -0.2598630487918854\n",
      "step: 641001, loss: -0.23477254807949066\n",
      "step: 642001, loss: -0.2778703272342682\n",
      "step: 643001, loss: -0.25061044096946716\n",
      "step: 644001, loss: -0.21595144271850586\n",
      "step: 645001, loss: -0.397989958524704\n",
      "step: 646001, loss: -0.2719065546989441\n",
      "step: 647001, loss: -0.16556118428707123\n",
      "step: 648001, loss: -0.3020675778388977\n",
      "step: 649001, loss: -0.3629315197467804\n",
      "step: 650001, loss: -0.29827314615249634\n",
      "step: 651001, loss: -0.347188264131546\n",
      "step: 652001, loss: -0.21290390193462372\n",
      "step: 653001, loss: -0.14652247726917267\n",
      "step: 654001, loss: -0.24481090903282166\n",
      "step: 655001, loss: -0.19747623801231384\n",
      "step: 656001, loss: -0.2128378301858902\n",
      "step: 657001, loss: -0.3436828851699829\n",
      "step: 658001, loss: -0.360892653465271\n",
      "step: 659001, loss: -0.36256399750709534\n",
      "step: 660001, loss: -0.256458044052124\n",
      "step: 661001, loss: -0.23035451769828796\n",
      "step: 662001, loss: -0.3943272829055786\n",
      "step: 663001, loss: -0.24935714900493622\n",
      "step: 664001, loss: -0.28560560941696167\n",
      "step: 665001, loss: -0.42375126481056213\n",
      "step: 666001, loss: -0.30350029468536377\n",
      "step: 667001, loss: -0.3338424265384674\n",
      "step: 668001, loss: -0.20323601365089417\n",
      "step: 669001, loss: -0.27047020196914673\n",
      "step: 670001, loss: -0.28436875343322754\n",
      "step: 671001, loss: -0.17365850508213043\n",
      "step: 672001, loss: -0.30657169222831726\n",
      "step: 673001, loss: -0.27828162908554077\n",
      "step: 674001, loss: -0.3611982762813568\n",
      "step: 675001, loss: -0.1678992062807083\n",
      "step: 676001, loss: -0.30719026923179626\n",
      "step: 677001, loss: -0.38445034623146057\n",
      "step: 678001, loss: -0.375458687543869\n",
      "step: 679001, loss: -0.34526023268699646\n",
      "step: 680001, loss: -0.24618849158287048\n",
      "step: 681001, loss: -0.2856721580028534\n",
      "step: 682001, loss: -0.29561036825180054\n",
      "step: 683001, loss: -0.2147235870361328\n",
      "step: 684001, loss: -0.210480198264122\n",
      "step: 685001, loss: -0.23620682954788208\n",
      "step: 686001, loss: -0.28393125534057617\n",
      "step: 687001, loss: -0.2960880994796753\n",
      "step: 688001, loss: -0.17436136305332184\n",
      "step: 689001, loss: -0.2843613922595978\n",
      "step: 690001, loss: -0.28541097044944763\n",
      "step: 691001, loss: -0.24973642826080322\n",
      "step: 692001, loss: -0.27739590406417847\n",
      "step: 693001, loss: -0.3442845642566681\n",
      "step: 694001, loss: -0.316291481256485\n",
      "step: 695001, loss: -0.37334030866622925\n",
      "step: 696001, loss: -0.191316619515419\n",
      "step: 697001, loss: -0.31594452261924744\n",
      "step: 698001, loss: -0.30196669697761536\n",
      "step: 699001, loss: -0.35087740421295166\n",
      "step: 700001, loss: -0.12701793015003204\n",
      "step: 701001, loss: -0.28756049275398254\n",
      "step: 702001, loss: -0.23019611835479736\n",
      "step: 703001, loss: -0.1977325677871704\n",
      "step: 704001, loss: -0.3109613060951233\n",
      "step: 705001, loss: -0.35923779010772705\n",
      "step: 706001, loss: -0.3259206712245941\n",
      "step: 707001, loss: -0.33084407448768616\n",
      "step: 708001, loss: -0.29227402806282043\n",
      "step: 709001, loss: -0.3767034113407135\n",
      "step: 710001, loss: -0.2405155897140503\n",
      "step: 711001, loss: -0.33524149656295776\n",
      "step: 712001, loss: -0.32934337854385376\n",
      "step: 713001, loss: -0.3296365737915039\n",
      "step: 714001, loss: -0.2920326292514801\n",
      "step: 715001, loss: -0.15136343240737915\n",
      "step: 716001, loss: -0.2685624063014984\n",
      "step: 717001, loss: -0.2735036015510559\n",
      "step: 718001, loss: -0.23164260387420654\n",
      "step: 719001, loss: -0.14551343023777008\n",
      "step: 720001, loss: -0.3211193382740021\n",
      "step: 721001, loss: -0.38485831022262573\n",
      "step: 722001, loss: -0.2676369547843933\n",
      "step: 723001, loss: -0.3071701228618622\n",
      "step: 724001, loss: -0.28292474150657654\n",
      "step: 725001, loss: -0.3199232816696167\n",
      "step: 726001, loss: -0.28321611881256104\n",
      "step: 727001, loss: -0.32841241359710693\n",
      "step: 728001, loss: -0.3179064095020294\n",
      "step: 729001, loss: -0.35800448060035706\n",
      "step: 730001, loss: -0.48216986656188965\n",
      "step: 731001, loss: -0.37419673800468445\n",
      "step: 732001, loss: -0.24121326208114624\n",
      "step: 733001, loss: -0.21541881561279297\n",
      "step: 734001, loss: -0.36648187041282654\n",
      "step: 735001, loss: -0.19437341392040253\n",
      "step: 736001, loss: -0.3260668218135834\n",
      "step: 737001, loss: -0.32485443353652954\n",
      "step: 738001, loss: -0.29633885622024536\n",
      "step: 739001, loss: -0.17101088166236877\n",
      "step: 740001, loss: -0.2914835810661316\n",
      "step: 741001, loss: -0.2495991587638855\n",
      "step: 742001, loss: -0.2398509979248047\n",
      "step: 743001, loss: -0.33525440096855164\n",
      "step: 744001, loss: -0.2313283383846283\n",
      "step: 745001, loss: -0.2992628514766693\n",
      "step: 746001, loss: -0.32319414615631104\n",
      "step: 747001, loss: -0.32039058208465576\n",
      "step: 748001, loss: -0.2895093858242035\n",
      "step: 749001, loss: -0.27108126878738403\n",
      "step: 750001, loss: -0.25293809175491333\n",
      "step: 751001, loss: -0.2602211833000183\n",
      "step: 752001, loss: -0.3129829466342926\n",
      "step: 753001, loss: -0.274046927690506\n",
      "step: 754001, loss: -0.34725844860076904\n",
      "step: 755001, loss: -0.18863952159881592\n",
      "step: 756001, loss: -0.2377699911594391\n",
      "step: 757001, loss: -0.3084341287612915\n",
      "step: 758001, loss: -0.3537323772907257\n",
      "step: 759001, loss: -0.19317474961280823\n",
      "step: 760001, loss: -0.2678437829017639\n",
      "step: 761001, loss: -0.4025634527206421\n",
      "step: 762001, loss: -0.346357524394989\n",
      "step: 763001, loss: -0.24358852207660675\n",
      "step: 764001, loss: -0.2536495625972748\n",
      "step: 765001, loss: -0.23047985136508942\n",
      "step: 766001, loss: -0.3430708646774292\n",
      "step: 767001, loss: -0.3052375912666321\n",
      "step: 768001, loss: -0.3057580292224884\n",
      "step: 769001, loss: -0.29190027713775635\n",
      "step: 770001, loss: -0.3001136779785156\n",
      "step: 771001, loss: -0.2792482376098633\n",
      "step: 772001, loss: -0.3328370153903961\n",
      "step: 773001, loss: -0.40267062187194824\n",
      "step: 774001, loss: -0.3017079830169678\n",
      "step: 775001, loss: -0.3448091745376587\n",
      "step: 776001, loss: -0.16609373688697815\n",
      "step: 777001, loss: -0.3734596371650696\n",
      "step: 778001, loss: -0.21047388017177582\n",
      "step: 779001, loss: -0.3451269268989563\n",
      "step: 780001, loss: -0.1989879459142685\n",
      "step: 781001, loss: -0.4472413957118988\n",
      "step: 782001, loss: -0.3101969063282013\n",
      "step: 783001, loss: -0.20214280486106873\n",
      "step: 784001, loss: -0.2763316035270691\n",
      "step: 785001, loss: -0.29930275678634644\n",
      "step: 786001, loss: -0.2820718288421631\n",
      "step: 787001, loss: -0.22603578865528107\n",
      "step: 788001, loss: -0.22878320515155792\n",
      "step: 789001, loss: -0.2531823217868805\n",
      "step: 790001, loss: -0.3123268783092499\n",
      "step: 791001, loss: -0.30116766691207886\n",
      "step: 792001, loss: -0.3747689127922058\n",
      "step: 793001, loss: -0.3924351930618286\n",
      "step: 794001, loss: -0.30263733863830566\n",
      "step: 795001, loss: -0.24553841352462769\n",
      "step: 796001, loss: -0.371819406747818\n",
      "step: 797001, loss: -0.27799686789512634\n",
      "step: 798001, loss: -0.19496722519397736\n",
      "step: 799001, loss: -0.14907647669315338\n",
      "step: 800001, loss: -0.31192219257354736\n",
      "step: 801001, loss: -0.30940720438957214\n",
      "step: 802001, loss: -0.4018853008747101\n",
      "step: 803001, loss: -0.3038260340690613\n",
      "step: 804001, loss: -0.3247535228729248\n",
      "step: 805001, loss: -0.23914185166358948\n",
      "step: 806001, loss: -0.25752270221710205\n",
      "step: 807001, loss: -0.1134030818939209\n",
      "step: 808001, loss: -0.3030684292316437\n",
      "step: 809001, loss: -0.3648303151130676\n",
      "step: 810001, loss: -0.39676132798194885\n",
      "step: 811001, loss: -0.2733960449695587\n",
      "step: 812001, loss: -0.26403507590293884\n",
      "step: 813001, loss: -0.2824564278125763\n",
      "step: 814001, loss: -0.24660970270633698\n",
      "step: 815001, loss: -0.29361146688461304\n",
      "step: 816001, loss: -0.29548972845077515\n",
      "step: 817001, loss: -0.24716521799564362\n",
      "step: 818001, loss: -0.23658809065818787\n",
      "step: 819001, loss: -0.31030091643333435\n",
      "step: 820001, loss: -0.3190549612045288\n",
      "step: 821001, loss: -0.2539665400981903\n",
      "step: 822001, loss: -0.2732992172241211\n",
      "step: 823001, loss: -0.3322025537490845\n",
      "step: 824001, loss: -0.2051958441734314\n",
      "step: 825001, loss: -0.3260822296142578\n",
      "step: 826001, loss: -0.3028501570224762\n",
      "step: 827001, loss: -0.4113309979438782\n",
      "step: 828001, loss: -0.3539196252822876\n",
      "step: 829001, loss: -0.30885255336761475\n",
      "step: 830001, loss: -0.24131855368614197\n",
      "step: 831001, loss: -0.2752014696598053\n",
      "step: 832001, loss: -0.24457722902297974\n",
      "step: 833001, loss: -0.2753641605377197\n",
      "step: 834001, loss: -0.20110464096069336\n",
      "step: 835001, loss: -0.30841436982154846\n",
      "step: 836001, loss: -0.21027067303657532\n",
      "step: 837001, loss: -0.2935393452644348\n",
      "step: 838001, loss: -0.2784254550933838\n",
      "step: 839001, loss: -0.30744850635528564\n",
      "step: 840001, loss: -0.3871941864490509\n",
      "step: 841001, loss: -0.21820375323295593\n",
      "step: 842001, loss: -0.30856654047966003\n",
      "step: 843001, loss: -0.2806154191493988\n",
      "step: 844001, loss: -0.3018847703933716\n",
      "step: 845001, loss: -0.18270500004291534\n",
      "step: 846001, loss: -0.27095967531204224\n",
      "step: 847001, loss: -0.3507627844810486\n",
      "step: 848001, loss: -0.17838582396507263\n",
      "step: 849001, loss: -0.26654252409935\n",
      "step: 850001, loss: -0.3751069903373718\n",
      "step: 851001, loss: -0.36643803119659424\n",
      "step: 852001, loss: -0.20979604125022888\n",
      "step: 853001, loss: -0.2125183492898941\n",
      "step: 854001, loss: -0.37357157468795776\n",
      "step: 855001, loss: -0.22949983179569244\n",
      "step: 856001, loss: -0.29297447204589844\n",
      "step: 886001, loss: -0.3364205062389374\n",
      "step: 887001, loss: -0.33583125472068787\n",
      "step: 888001, loss: -0.21701551973819733\n",
      "step: 889001, loss: -0.2820369005203247\n",
      "step: 890001, loss: -0.2810324430465698\n",
      "step: 891001, loss: -0.2917640507221222\n",
      "step: 892001, loss: -0.38097506761550903\n",
      "step: 893001, loss: -0.29158732295036316\n",
      "step: 894001, loss: -0.28317439556121826\n",
      "step: 895001, loss: -0.2257070690393448\n",
      "step: 896001, loss: -0.4082936942577362\n",
      "step: 897001, loss: -0.26981034874916077\n",
      "step: 898001, loss: -0.4324587285518646\n",
      "step: 899001, loss: -0.268568754196167\n",
      "step: 900001, loss: -0.3180121183395386\n",
      "step: 901001, loss: -0.23208527266979218\n",
      "step: 902001, loss: -0.2891373336315155\n",
      "step: 903001, loss: -0.32152488827705383\n",
      "step: 904001, loss: -0.22483281791210175\n",
      "step: 905001, loss: -0.2506016790866852\n",
      "step: 906001, loss: -0.2996390163898468\n",
      "step: 907001, loss: -0.297725647687912\n",
      "step: 908001, loss: -0.2374948263168335\n",
      "step: 909001, loss: -0.34784674644470215\n",
      "step: 910001, loss: -0.30154573917388916\n",
      "step: 911001, loss: -0.3604004681110382\n",
      "step: 912001, loss: -0.2571360468864441\n",
      "step: 913001, loss: -0.30628207325935364\n",
      "step: 914001, loss: -0.35924896597862244\n",
      "step: 915001, loss: -0.26318854093551636\n",
      "step: 916001, loss: -0.21861524879932404\n",
      "step: 917001, loss: -0.31364449858665466\n",
      "step: 918001, loss: -0.24144811928272247\n",
      "step: 919001, loss: -0.18263214826583862\n",
      "step: 920001, loss: -0.19926568865776062\n",
      "step: 921001, loss: -0.24558712542057037\n",
      "step: 922001, loss: -0.4316674470901489\n",
      "step: 923001, loss: -0.3165391981601715\n",
      "step: 924001, loss: -0.26450929045677185\n",
      "step: 925001, loss: -0.2084091752767563\n",
      "step: 926001, loss: -0.285780668258667\n",
      "step: 927001, loss: -0.198626309633255\n",
      "step: 928001, loss: -0.3863949477672577\n",
      "step: 929001, loss: -0.36129799485206604\n",
      "step: 930001, loss: -0.2090229094028473\n",
      "step: 931001, loss: -0.33008015155792236\n",
      "step: 932001, loss: -0.2816559672355652\n",
      "step: 933001, loss: -0.3754516839981079\n",
      "step: 934001, loss: -0.3656383454799652\n",
      "step: 935001, loss: -0.24412250518798828\n",
      "step: 936001, loss: -0.21111828088760376\n",
      "step: 937001, loss: -0.33156654238700867\n",
      "step: 938001, loss: -0.25882214307785034\n",
      "step: 939001, loss: -0.23272500932216644\n",
      "step: 940001, loss: -0.27155619859695435\n",
      "step: 941001, loss: -0.3699605464935303\n",
      "step: 942001, loss: -0.25924059748649597\n",
      "step: 943001, loss: -0.3117178678512573\n",
      "step: 944001, loss: -0.23997168242931366\n",
      "step: 945001, loss: -0.2977907955646515\n",
      "step: 946001, loss: -0.28884923458099365\n",
      "step: 947001, loss: -0.23984357714653015\n",
      "step: 948001, loss: -0.19302424788475037\n",
      "step: 949001, loss: -0.31041237711906433\n",
      "step: 950001, loss: -0.37432411313056946\n",
      "step: 951001, loss: -0.29897332191467285\n",
      "step: 952001, loss: -0.32634949684143066\n",
      "step: 953001, loss: -0.3195525109767914\n",
      "step: 954001, loss: -0.30282875895500183\n",
      "step: 955001, loss: -0.3240699768066406\n",
      "step: 956001, loss: -0.35928285121917725\n",
      "step: 957001, loss: -0.21401004493236542\n",
      "step: 958001, loss: -0.2872462570667267\n",
      "step: 959001, loss: -0.33414462208747864\n",
      "step: 960001, loss: -0.2778129279613495\n",
      "step: 961001, loss: -0.36273956298828125\n",
      "step: 962001, loss: -0.18105676770210266\n",
      "step: 963001, loss: -0.24825473129749298\n",
      "step: 964001, loss: -0.34953323006629944\n",
      "step: 965001, loss: -0.28830990195274353\n",
      "step: 966001, loss: -0.20317983627319336\n",
      "step: 967001, loss: -0.29958680272102356\n",
      "step: 968001, loss: -0.35070160031318665\n",
      "step: 969001, loss: -0.2898821532726288\n",
      "step: 970001, loss: -0.24341486394405365\n",
      "step: 971001, loss: -0.26703718304634094\n",
      "step: 972001, loss: -0.24982567131519318\n",
      "step: 973001, loss: -0.3361281454563141\n",
      "step: 974001, loss: -0.2983475923538208\n",
      "step: 975001, loss: -0.2979719340801239\n",
      "step: 976001, loss: -0.29527902603149414\n",
      "step: 977001, loss: -0.33444496989250183\n",
      "step: 978001, loss: -0.2678678333759308\n",
      "step: 979001, loss: -0.1941109597682953\n",
      "step: 980001, loss: -0.26838433742523193\n",
      "step: 981001, loss: -0.24185024201869965\n",
      "step: 982001, loss: -0.28440961241722107\n",
      "step: 983001, loss: -0.21587541699409485\n",
      "step: 984001, loss: -0.25511467456817627\n",
      "step: 985001, loss: -0.2896859049797058\n",
      "step: 986001, loss: -0.19390608370304108\n",
      "step: 987001, loss: -0.13052022457122803\n",
      "step: 988001, loss: -0.26132309436798096\n",
      "step: 989001, loss: -0.2858058214187622\n",
      "step: 990001, loss: -0.2431318163871765\n",
      "step: 991001, loss: -0.3026164174079895\n",
      "step: 992001, loss: -0.29746541380882263\n",
      "step: 993001, loss: -0.3379560112953186\n",
      "step: 994001, loss: -0.3577748239040375\n",
      "step: 995001, loss: -0.27586784958839417\n",
      "step: 996001, loss: -0.2551257014274597\n",
      "step: 997001, loss: -0.35603779554367065\n",
      "step: 998001, loss: -0.2753336429595947\n",
      "step: 999001, loss: -0.22640280425548553\n",
      "step: 1000001, loss: -0.37497830390930176\n",
      "step: 1001001, loss: -0.24430954456329346\n",
      "step: 1002001, loss: -0.3504781723022461\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9b2a244cac75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#     session.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-9b2a244cac75>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(iters, tain_op)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mstep_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step: {step}, loss: {loss}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(10)\n",
    "\n",
    "ds = get_train_inputs(data_loader, batch_size=200)\n",
    "iter = tf.data.Iterator.from_structure(ds.output_types,ds.output_shapes)\n",
    "training_init_op = iter.make_initializer(ds)\n",
    "batch=iter.get_next()\n",
    "\n",
    "global_step_tensor = tf.Variable(0, trainable=False, name='global_step')\n",
    "y_size = 1\n",
    "\n",
    "y=batch['y']\n",
    "x=batch['x']\n",
    "x_size=1\n",
    "\n",
    "mode=tf.estimator.ModeKeys.TRAIN\n",
    "params={}\n",
    "params['hidden_units'] = 10\n",
    "params['learning_rate'] = 0.001\n",
    "params['num_layers'] = 2\n",
    "params['k_mix'] = 10\n",
    "params['cov_type'] = 'const'\n",
    "\n",
    "k_mix = params[\"k_mix\"]\n",
    "num_layers = params[\"num_layers\"]\n",
    "hidden_units = params['hidden_units']\n",
    "learning_rate = params[\"learning_rate\"]\n",
    "cov_type = params[\"cov_type\"]\n",
    "\n",
    "layer = x\n",
    "\n",
    "n_out = k_mix * 3 * y_size  # pi, mu, stdev\n",
    "\n",
    "if x is not None:\n",
    "    for i in range(num_layers):\n",
    "        layer = tf.layers.dense(layer, units=hidden_units, activation=tf.nn.tanh, name=\"h_layer_%d\" % i)\n",
    "\n",
    "    Wo = tf.get_variable(\"Wo\", shape=[params['hidden_units'], n_out], dtype=tf.float32)\n",
    "    bo = tf.get_variable(\"bo\", shape=[1, n_out], dtype=tf.float32)\n",
    "\n",
    "    output = tf.matmul(layer, Wo) + bo\n",
    "else:\n",
    "    output = tf.get_variable(\"output\", shape=(1, n_out))\n",
    "\n",
    "mixtures = get_mixtures(output, y_size)\n",
    "\n",
    "marginal_lls = compute_lls(mixtures, y)\n",
    "ll = log_likelihood(cov_type, mixtures, y, marginal_lls, x, params)\n",
    "loss = get_loss(ll)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
    "\n",
    "train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "def train_loop(iters, tain_op):\n",
    "    session.run(training_init_op)\n",
    "    \n",
    "    print(\"step, loss: \",session.run([tf.train.get_global_step(),loss]))\n",
    "  \n",
    "    for step in range(iters):  \n",
    "        step_val,loss_val,_=session.run([tf.train.get_global_step(),loss,tain_op])\n",
    "        if step % 1000 ==0:\n",
    "            print(\"step: {step}, loss: {loss}\".format(step=step_val,loss=loss_val))\n",
    "        \n",
    "    print(\"loss: \",session.run(loss))\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.04))) as session:\n",
    "#     session = tf_debug.LocalCLIDebugWrapperSession(session)\n",
    "#     session.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
    "    session.run(tf.global_variables_initializer())  \n",
    "    train_loop(10000000, train_op)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
