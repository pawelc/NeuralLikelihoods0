{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from utils import create_session_config\n",
    "import tensorflow_probability as tp\n",
    "import numpy as np\n",
    "from models.utils import covariance,correlation_matrix,tf_cov,assert_cov_positive_definite,corr,print_tensor,print_tensors,constrain_cdf\n",
    "from models.nn_pdf_common import transform_x,density_estimator,create_pdf_layer_mv,create_partially_monotone_dense_layer,create_monotone_dense_layer,create_positive_weights\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from flags import FLAGS\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import os\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cdf_transform(x, y, params, positive_transform):\n",
    "#     x_transform = transform_x(params, x)\n",
    "  \n",
    "#     ys = []\n",
    "#     xys = []\n",
    "#     for i in range(y.shape[1].value):\n",
    "#         ys.append(tf.slice(y, [0, i], [-1, 1]) )\n",
    "#         xys.append(tf.concat([x_transform, ys[-1]], axis=1))\n",
    "    \n",
    "#     activation1 = tf.nn.sigmoid\n",
    "#     activation2 = tf.nn.relu\n",
    "\n",
    "#     arch2 = params['arch2']\n",
    "#     arch3 = params['arch3']\n",
    "\n",
    "#     with tf.variable_scope(\"cdf_transform\"):\n",
    "#         for i in range(len(ys)):\n",
    "#             with tf.variable_scope(\"y_comp_%d\"%i):\n",
    "#                 with tf.variable_scope(\"xy%d_partially_monotone\"%i):\n",
    "#                     xys[i] = create_partially_monotone_dense_layer(xys[i], arch2[0], x_transform.shape[-1].value,\n",
    "#                                                                 1, activation=activation1, positive_transform=positive_transform)\n",
    "\n",
    "#                 for j, units in enumerate(arch2[1:-1]):\n",
    "#                     with tf.variable_scope(\"xy%d_l_%d\"%(i,j)):\n",
    "#                         xys[i] = create_monotone_dense_layer(xys[i], units, activation=activation1, positive_transform=positive_transform)\n",
    "\n",
    "#                 with tf.variable_scope(\"xy1_l_%d\"%len(arch2)):\n",
    "#                     xy1_prod = create_monotone_dense_layer(xy1, arch2[-1], activation=activation1, positive_transform=positive_transform)\n",
    "\n",
    "#                 with tf.variable_scope(\"xy2_partially_monotone\"):\n",
    "#                     xy2 = create_partially_monotone_dense_layer(xy2, arch2[0], x_transform.shape[-1].value,\n",
    "#                                                                 1, activation=activation1, positive_transform=positive_transform)\n",
    "\n",
    "#                 for i, units in enumerate(arch2[1:-1]):\n",
    "#                     with tf.variable_scope(\"xy2_l_%d\"%i):\n",
    "#                         xy2 = create_monotone_dense_layer(xy2, units, activation=activation1, positive_transform=positive_transform)\n",
    "\n",
    "#                 with tf.variable_scope(\"xy2_l_%d\"%len(arch2)):\n",
    "#                     xy2_prod = create_monotone_dense_layer(xy2, arch2[-1], activation=activation1, positive_transform=positive_transform)\n",
    "  \n",
    "\n",
    "#         with tf.variable_scope(\"xy_prod\"):\n",
    "#             xy_prefinal = tf.multiply(xy1_prod, xy2_prod)\n",
    "#             xy_prefinal = create_monotone_dense_layer(xy_prefinal, arch3[0], activation=activation2,\n",
    "#                                                       positive_transform=positive_transform)\n",
    "\n",
    "#         for i, units in enumerate(arch3[1:]):\n",
    "#             with tf.variable_scope(\"xy_prefinal_l_%d\"%i):\n",
    "#                 xy_prefinal = create_monotone_dense_layer(xy_prefinal, units, activation=activation2,\n",
    "#                                                       positive_transform=positive_transform)\n",
    "\n",
    "#         return xy_prefinal,create_monotone_dense_layer(xy_prefinal, 1, activation=activation2,\n",
    "#                                            positive_transform=positive_transform)\n",
    "\n",
    "def cdf_transform(x, y, params, positive_transform):\n",
    "    x_transform = transform_x(params, x)\n",
    "\n",
    "    y1 = tf.slice(y, [0, 0], [-1, 1])\n",
    "\n",
    "    y2 = tf.slice(y, [0, 1], [-1, 1])\n",
    "\n",
    "    xy1 = tf.concat([x_transform, y1], axis=1)\n",
    "    xy2 = tf.concat([x_transform, y2], axis=1)\n",
    "\n",
    "    activation1 = tf.nn.sigmoid\n",
    "    activation2 = tf.nn.relu\n",
    "\n",
    "    arch2 = params['arch2']\n",
    "    arch3 = params['arch3']\n",
    "    #     relu_bias_initializer = float(params[\"relu_bias_initializer\"])\n",
    "\n",
    "    with tf.variable_scope(\"cdf_transform\"):\n",
    "\n",
    "        with tf.variable_scope(\"xy1_partially_monotone\"):\n",
    "            xy1 = create_partially_monotone_dense_layer(xy1, arch2[0], x_transform.shape[-1].value,\n",
    "                                                        1, activation=activation1,\n",
    "                                                        positive_transform=positive_transform)\n",
    "\n",
    "        for i, units in enumerate(arch2[1:-1]):\n",
    "            with tf.variable_scope(\"xy1_l_%d\" % i):\n",
    "                xy1 = create_monotone_dense_layer(xy1, units, activation=activation1,\n",
    "                                                  positive_transform=positive_transform)\n",
    "\n",
    "        with tf.variable_scope(\"xy1_l_%d\" % len(arch2)):\n",
    "            xy1_prod = create_monotone_dense_layer(xy1, arch2[-1], activation=activation1,\n",
    "                                                   positive_transform=positive_transform)\n",
    "\n",
    "        #         with tf.variable_scope(\"xy1_add_l_%d\"%len(arch2)):\n",
    "        #             xy1_add = create_monotone_dense_layer(xy1, arch2[-1], activation=activation1, positive_transform=positive_transform)\n",
    "\n",
    "        with tf.variable_scope(\"xy2_partially_monotone\"):\n",
    "            xy2 = create_partially_monotone_dense_layer(xy2, arch2[0], x_transform.shape[-1].value,\n",
    "                                                        1, activation=activation1,\n",
    "                                                        positive_transform=positive_transform)\n",
    "\n",
    "        for i, units in enumerate(arch2[1:-1]):\n",
    "            with tf.variable_scope(\"xy2_l_%d\" % i):\n",
    "                xy2 = create_monotone_dense_layer(xy2, units, activation=activation1,\n",
    "                                                  positive_transform=positive_transform)\n",
    "\n",
    "        with tf.variable_scope(\"xy2_l_%d\" % len(arch2)):\n",
    "            xy2_prod = create_monotone_dense_layer(xy2, arch2[-1], activation=activation1,\n",
    "                                                   positive_transform=positive_transform)\n",
    "\n",
    "        #         with tf.variable_scope(\"xy2_add_l_%d\"%len(arch2)):\n",
    "        #             xy2_add = create_monotone_dense_layer(xy2, arch2[-1], activation=activation1, positive_transform=positive_transform)\n",
    "\n",
    "        with tf.variable_scope(\"xy_prod\"):\n",
    "            xy_prefinal = tf.multiply(xy1_prod, xy2_prod)\n",
    "            #             tf.concat([xy_prefinal, xy1_add,xy2_add], axis=1)\n",
    "            xy_prefinal = create_monotone_dense_layer(xy_prefinal, arch3[0], activation=activation2,\n",
    "                                                      positive_transform=positive_transform)\n",
    "\n",
    "        for i, units in enumerate(arch3[1:]):\n",
    "            with tf.variable_scope(\"xy_prefinal_l_%d\" % i):\n",
    "                xy_prefinal = create_monotone_dense_layer(xy_prefinal, units, activation=activation2,\n",
    "                                                          positive_transform=positive_transform)\n",
    "\n",
    "        return create_monotone_dense_layer(xy_prefinal, 1, activation=activation2,\n",
    "                                                        positive_transform=positive_transform)\n",
    "    \n",
    "def create_pdf_layer(cdf, y):\n",
    "    with tf.name_scope(\"pdf_layer\"):\n",
    "        assert cdf.shape[0].value == y.shape[0].value\n",
    "        assert y.shape[-1].value == 1\n",
    "        assert cdf.shape[-1].value == 1\n",
    "        # because it is list and cdf is only 1 element\n",
    "        return tf.gradients(cdf, y)[0]\n",
    "    \n",
    "def create_pdf_layer_mv(cdf, y_components):\n",
    "    gradients = cdf\n",
    "    for i, y in enumerate(y_components):\n",
    "        with tf.name_scope(\"pdf_layer_mv_%d\"%i):\n",
    "            gradients = create_pdf_layer(gradients, y)\n",
    "    return gradients\n",
    "\n",
    "def cdf_transform_normalizd(x, y, params, positive_transform, mode):\n",
    "    with tf.variable_scope(\"extreme_vals\",reuse=tf.AUTO_REUSE):\n",
    "        y_max_values_vars = tf.get_variable(\"y_max_values_var\", trainable=False, dtype=tf.float32,\n",
    "                                            shape=(1, y.shape[-1].value), initializer=tf.constant_initializer(np.nan))\n",
    "        y_min_values_vars = tf.get_variable(\"y_min_values_var\", trainable=False, dtype=tf.float32,\n",
    "                                            shape=(1, y.shape[-1].value), initializer=tf.constant_initializer(np.nan))\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            y_max_values = tf.reduce_max(y, axis=0, keepdims=True)\n",
    "            y_max_values_vars = tf.cond(tf.reduce_all(tf.is_nan(y_max_values_vars)), \\\n",
    "                  lambda: tf.assign(y_max_values_vars, y_max_values), \\\n",
    "                  lambda: tf.assign(y_max_values_vars, tf.reduce_max(tf.concat([y_max_values, y_max_values_vars], axis=0),axis=0, keepdims=True)))\n",
    "\n",
    "            y_min_values = tf.reduce_min(y, axis=0, keepdims=True)\n",
    "            y_min_values_vars = tf.cond(tf.reduce_all(tf.is_nan(y_min_values_vars)), \\\n",
    "                  lambda: tf.assign(y_min_values_vars, y_min_values), \\\n",
    "                  lambda: tf.assign(y_min_values_vars, tf.reduce_max(tf.concat([y_min_values, y_min_values_vars], axis=0),axis=0, keepdims=True)))\n",
    "\n",
    "    with tf.variable_scope(\"cdf\", reuse=tf.AUTO_REUSE):\n",
    "        xy_prefinal,non_normalized_cdf = cdf_transform(x, y, params, positive_transform)\n",
    "        _,cdf_normalization = cdf_transform(x, tf.tile(y_max_values_vars, [tf.shape(x)[0], 1]),\n",
    "                                                       params, positive_transform)\n",
    "    return xy_prefinal,non_normalized_cdf,cdf_normalization,y_min_values_vars, y_max_values_vars,print_tensor(tf.div(non_normalized_cdf, tf.stop_gradient(cdf_normalization)), name=\"cdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_cov: [[49.       44.1     ]\n",
      " [44.100002 81.      ]]\n",
      "step, loss and true_loss:  [0, 9.849984, 6.667027]\n",
      "step: 1, loss: 9.781632423400879\n",
      "step: 101, loss: 9.136486053466797\n",
      "step: 201, loss: 8.553621292114258\n",
      "step: 301, loss: 8.248153686523438\n",
      "step: 401, loss: 8.042720794677734\n",
      "step: 501, loss: 7.977960109710693\n",
      "step: 601, loss: 7.873007297515869\n",
      "step: 701, loss: 7.74100399017334\n",
      "step: 801, loss: 7.676098346710205\n",
      "step: 901, loss: 7.535309314727783\n",
      "step: 1001, loss: 7.524650573730469\n",
      "step: 1101, loss: 7.48321008682251\n",
      "step: 1201, loss: 7.351883888244629\n",
      "step: 1301, loss: 7.278109073638916\n",
      "step: 1401, loss: 7.245182991027832\n",
      "step: 1501, loss: 7.20064115524292\n",
      "step: 1601, loss: 7.14904260635376\n",
      "step: 1701, loss: 7.099614143371582\n",
      "step: 1801, loss: 7.052713871002197\n",
      "step: 1901, loss: 7.116256237030029\n",
      "step: 2001, loss: 7.01458740234375\n",
      "step: 2101, loss: 6.999449729919434\n",
      "step: 2201, loss: 6.9806952476501465\n",
      "step: 2301, loss: 6.973982334136963\n",
      "step: 2401, loss: 6.988390922546387\n",
      "step: 2501, loss: 6.922163486480713\n",
      "step: 2601, loss: 6.927504062652588\n",
      "step: 2701, loss: 6.897383689880371\n",
      "step: 2801, loss: 6.8780694007873535\n",
      "step: 2901, loss: 6.8781819343566895\n",
      "step: 3001, loss: 6.897799491882324\n",
      "step: 3101, loss: 6.8554534912109375\n",
      "step: 3201, loss: 6.840131759643555\n",
      "step: 3301, loss: 6.82696008682251\n",
      "step: 3401, loss: 6.849495887756348\n",
      "step: 3501, loss: 6.8992462158203125\n",
      "step: 3601, loss: 6.812352657318115\n",
      "step: 3701, loss: 6.827751636505127\n",
      "step: 3801, loss: 6.771620750427246\n",
      "step: 3901, loss: 6.780153274536133\n",
      "step: 4001, loss: 6.744295597076416\n",
      "step: 4101, loss: 6.771768569946289\n",
      "step: 4201, loss: 6.815324783325195\n",
      "step: 4301, loss: 6.794965744018555\n",
      "step: 4401, loss: 6.744228363037109\n",
      "step: 4501, loss: 6.792482852935791\n",
      "step: 4601, loss: 6.641544818878174\n",
      "step: 4701, loss: 6.765838146209717\n",
      "step: 4801, loss: 6.818388938903809\n",
      "step: 4901, loss: 6.705172538757324\n",
      "step: 5001, loss: 6.801207065582275\n",
      "step: 5101, loss: 6.738704681396484\n",
      "step: 5201, loss: 6.728579998016357\n",
      "step: 5301, loss: 6.692432403564453\n",
      "step: 5401, loss: 6.678127765655518\n",
      "step: 5501, loss: 6.750547885894775\n",
      "step: 5601, loss: 6.723117351531982\n",
      "step: 5701, loss: 6.674036026000977\n",
      "step: 5801, loss: 6.689767837524414\n",
      "step: 5901, loss: 6.673937797546387\n",
      "step: 6001, loss: 6.672209739685059\n",
      "step: 6101, loss: 6.717790603637695\n",
      "step: 6201, loss: 6.713298797607422\n",
      "step: 6301, loss: 6.761474609375\n",
      "step: 6401, loss: 6.696788311004639\n",
      "step: 6501, loss: 6.754787921905518\n",
      "step: 6601, loss: 6.688013553619385\n",
      "step: 6701, loss: 6.715365886688232\n",
      "step: 6801, loss: 6.697521209716797\n",
      "step: 6901, loss: 6.672049045562744\n",
      "step: 7001, loss: 6.629796028137207\n",
      "step: 7101, loss: 6.655600547790527\n",
      "step: 7201, loss: 6.732883930206299\n",
      "step: 7301, loss: 6.716005802154541\n",
      "step: 7401, loss: 6.69303035736084\n",
      "step: 7501, loss: 6.723621845245361\n",
      "step: 7601, loss: 6.663640975952148\n",
      "step: 7701, loss: 6.688251495361328\n",
      "step: 7801, loss: 6.5967302322387695\n",
      "step: 7901, loss: 6.677919864654541\n",
      "step: 8001, loss: 6.691773414611816\n",
      "step: 8101, loss: 6.706107139587402\n",
      "step: 8201, loss: 6.675262928009033\n",
      "step: 8301, loss: 6.671098709106445\n",
      "step: 8401, loss: 6.649872303009033\n",
      "step: 8501, loss: 6.769721984863281\n",
      "step: 8601, loss: 6.662252902984619\n",
      "step: 8701, loss: 6.645247459411621\n",
      "step: 8801, loss: 6.753642559051514\n",
      "step: 8901, loss: 6.649392127990723\n",
      "step: 9001, loss: 6.713388442993164\n",
      "step: 9101, loss: 6.688633918762207\n",
      "step: 9201, loss: 6.617897033691406\n",
      "step: 9301, loss: 6.655577182769775\n",
      "step: 9401, loss: 6.68937873840332\n",
      "step: 9501, loss: 6.707963943481445\n",
      "step: 9601, loss: 6.720822334289551\n",
      "step: 9701, loss: 6.631265640258789\n",
      "step: 9801, loss: 6.654677867889404\n",
      "step: 9901, loss: 6.633958339691162\n",
      "step: 10001, loss: 6.6751508712768555\n",
      "step: 10101, loss: 6.706193447113037\n",
      "step: 10201, loss: 6.671536922454834\n",
      "step: 10301, loss: 6.705662250518799\n",
      "step: 10401, loss: 6.674966335296631\n",
      "step: 10501, loss: 6.665863990783691\n",
      "step: 10601, loss: 6.613722801208496\n",
      "step: 10701, loss: 6.6568169593811035\n",
      "step: 10801, loss: 6.700976371765137\n",
      "step: 10901, loss: 6.5889058113098145\n",
      "step: 11001, loss: 6.738551139831543\n",
      "step: 11101, loss: 6.706512928009033\n",
      "step: 11201, loss: 6.70440673828125\n",
      "step: 11301, loss: 6.627727031707764\n",
      "step: 11401, loss: 6.605902194976807\n",
      "step: 11501, loss: 6.72256326675415\n",
      "step: 11601, loss: 6.626724720001221\n",
      "step: 11701, loss: 6.6734442710876465\n",
      "step: 11801, loss: 6.65631103515625\n",
      "step: 11901, loss: 6.633111000061035\n",
      "step: 12001, loss: 6.660428524017334\n",
      "step: 12101, loss: 6.644748210906982\n",
      "step: 12201, loss: 6.63459587097168\n",
      "step: 12301, loss: 6.6796674728393555\n",
      "step: 12401, loss: 6.696205139160156\n",
      "step: 12501, loss: 6.64499568939209\n",
      "step: 12601, loss: 6.669490337371826\n",
      "step: 12701, loss: 6.635951519012451\n",
      "step: 12801, loss: 6.6812310218811035\n",
      "step: 12901, loss: 6.640893936157227\n",
      "step: 13001, loss: 6.627542972564697\n",
      "step: 13101, loss: 6.658960342407227\n",
      "step: 13201, loss: 6.637575149536133\n",
      "step: 13301, loss: 6.6182355880737305\n",
      "step: 13401, loss: 6.663687229156494\n",
      "step: 13501, loss: 6.658941268920898\n",
      "step: 13601, loss: 6.699512481689453\n",
      "step: 13701, loss: 6.631420135498047\n",
      "step: 13801, loss: 6.649378299713135\n",
      "step: 13901, loss: 6.690345764160156\n",
      "step: 14001, loss: 6.641735076904297\n",
      "step: 14101, loss: 6.681330680847168\n",
      "step: 14201, loss: 6.616758346557617\n",
      "step: 14301, loss: 6.674923896789551\n",
      "step: 14401, loss: 6.618321895599365\n",
      "step: 14501, loss: 6.6688103675842285\n",
      "step: 14601, loss: 6.644468307495117\n",
      "step: 14701, loss: 6.699583530426025\n",
      "step: 14801, loss: 6.682707786560059\n",
      "step: 14901, loss: 6.619468688964844\n",
      "step: 15001, loss: 6.640888690948486\n",
      "step: 15101, loss: 6.67612361907959\n",
      "step: 15201, loss: 6.650063991546631\n",
      "step: 15301, loss: 6.713136672973633\n",
      "step: 15401, loss: 6.565084457397461\n",
      "step: 15501, loss: 6.664565563201904\n",
      "step: 15601, loss: 6.669164657592773\n",
      "step: 15701, loss: 6.605571269989014\n",
      "step: 15801, loss: 6.7223687171936035\n",
      "step: 15901, loss: 6.689098834991455\n",
      "step: 16001, loss: 6.595181465148926\n",
      "step: 16101, loss: 6.68366003036499\n",
      "step: 16201, loss: 6.668720722198486\n",
      "step: 16301, loss: 6.615861892700195\n",
      "step: 16401, loss: 6.703184604644775\n",
      "step: 16501, loss: 6.63743257522583\n",
      "step: 16601, loss: 6.6225996017456055\n",
      "step: 16701, loss: 6.638001918792725\n",
      "step: 16801, loss: 6.672650337219238\n",
      "step: 16901, loss: 6.620064735412598\n",
      "step: 17001, loss: 6.628194332122803\n",
      "step: 17101, loss: 6.673559665679932\n",
      "step: 17201, loss: 6.654983043670654\n",
      "step: 17301, loss: 6.658804893493652\n",
      "step: 17401, loss: 6.66880464553833\n",
      "step: 17501, loss: 6.728198051452637\n",
      "step: 17601, loss: 6.635666847229004\n",
      "step: 17701, loss: 6.655722618103027\n",
      "step: 17801, loss: 6.678552627563477\n",
      "step: 17901, loss: 6.659478187561035\n",
      "step: 18001, loss: 6.706761360168457\n",
      "step: 18101, loss: 6.634973049163818\n",
      "step: 18201, loss: 6.633937835693359\n",
      "step: 18301, loss: 6.655427932739258\n",
      "step: 18401, loss: 6.639301300048828\n",
      "step: 18501, loss: 6.650073528289795\n",
      "step: 18601, loss: 6.669888496398926\n",
      "step: 18701, loss: 6.670557975769043\n",
      "step: 18801, loss: 6.683272361755371\n",
      "step: 18901, loss: 6.706362724304199\n",
      "step: 19001, loss: 6.691356182098389\n",
      "step: 19101, loss: 6.667296886444092\n",
      "step: 19201, loss: 6.624486923217773\n",
      "step: 19301, loss: 6.68507719039917\n",
      "step: 19401, loss: 6.615029335021973\n",
      "step: 19501, loss: 6.674685478210449\n",
      "step: 19601, loss: 6.705155849456787\n",
      "step: 19701, loss: 6.581124305725098\n",
      "step: 19801, loss: 6.654440402984619\n",
      "step: 19901, loss: 6.648247718811035\n",
      "step: 20001, loss: 6.687517166137695\n",
      "step: 20101, loss: 6.685052871704102\n",
      "step: 20201, loss: 6.623226642608643\n",
      "step: 20301, loss: 6.610535144805908\n",
      "step: 20401, loss: 6.67543363571167\n",
      "step: 20501, loss: 6.677789211273193\n",
      "step: 20601, loss: 6.65947961807251\n",
      "step: 20701, loss: 6.652952671051025\n",
      "step: 20801, loss: 6.670979022979736\n",
      "step: 20901, loss: 6.680517673492432\n",
      "step: 21001, loss: 6.662667751312256\n",
      "step: 21101, loss: 6.649018287658691\n",
      "step: 21201, loss: 6.647886753082275\n",
      "step: 21301, loss: 6.6780900955200195\n",
      "step: 21401, loss: 6.622989654541016\n",
      "step: 21501, loss: 6.638497352600098\n",
      "step: 21601, loss: 6.722148895263672\n",
      "step: 21701, loss: 6.617115020751953\n",
      "step: 21801, loss: 6.670811176300049\n",
      "step: 21901, loss: 6.678330898284912\n",
      "step: 22001, loss: 6.718591213226318\n",
      "step: 22101, loss: 6.69520902633667\n",
      "step: 22201, loss: 6.671851634979248\n",
      "step: 22301, loss: 6.668267726898193\n",
      "step: 22401, loss: 6.706815719604492\n",
      "step: 22501, loss: 6.6471333503723145\n",
      "step: 22601, loss: 6.63525390625\n",
      "step: 22701, loss: 6.664783954620361\n",
      "step: 22801, loss: 6.670804023742676\n",
      "step: 22901, loss: 6.624950408935547\n",
      "step: 23001, loss: 6.675796985626221\n",
      "step: 23101, loss: 6.652248382568359\n",
      "step: 23201, loss: 6.6201982498168945\n",
      "step: 23301, loss: 6.58380126953125\n",
      "step: 23401, loss: 6.607006072998047\n",
      "step: 23501, loss: 6.667029857635498\n",
      "step: 23601, loss: 6.629601955413818\n",
      "step: 23701, loss: 6.672304630279541\n",
      "step: 23801, loss: 6.693428993225098\n",
      "step: 23901, loss: 6.673823356628418\n",
      "step: 24001, loss: 6.67799711227417\n",
      "step: 24101, loss: 6.6598591804504395\n",
      "step: 24201, loss: 6.647640228271484\n",
      "step: 24301, loss: 6.6492600440979\n",
      "step: 24401, loss: 6.627050399780273\n",
      "step: 24501, loss: 6.64032506942749\n",
      "step: 24601, loss: 6.6523942947387695\n",
      "step: 24701, loss: 6.630270957946777\n",
      "step: 24801, loss: 6.659611225128174\n",
      "step: 24901, loss: 6.6675639152526855\n",
      "step: 25001, loss: 6.661532878875732\n",
      "step: 25101, loss: 6.601964950561523\n",
      "step: 25201, loss: 6.617607593536377\n",
      "step: 25301, loss: 6.6842217445373535\n",
      "step: 25401, loss: 6.690986156463623\n",
      "step: 25501, loss: 6.694863319396973\n",
      "step: 25601, loss: 6.623023509979248\n",
      "step: 25701, loss: 6.627578258514404\n",
      "step: 25801, loss: 6.638995170593262\n",
      "step: 25901, loss: 6.623409271240234\n",
      "step: 26001, loss: 6.595242500305176\n",
      "step: 26101, loss: 6.705195426940918\n",
      "step: 26201, loss: 6.664647579193115\n",
      "step: 26301, loss: 6.689781665802002\n",
      "step: 26401, loss: 6.5877838134765625\n",
      "step: 26501, loss: 6.676161289215088\n",
      "step: 26601, loss: 6.667343616485596\n",
      "step: 26701, loss: 6.616840839385986\n",
      "step: 26801, loss: 6.691061019897461\n",
      "step: 26901, loss: 6.6606316566467285\n",
      "step: 27001, loss: 6.646703720092773\n",
      "step: 27101, loss: 6.637269973754883\n",
      "step: 27201, loss: 6.673614025115967\n",
      "step: 27301, loss: 6.64829158782959\n",
      "step: 27401, loss: 6.727281093597412\n",
      "step: 27501, loss: 6.670287132263184\n",
      "step: 27601, loss: 6.607014179229736\n",
      "step: 27701, loss: 6.661899566650391\n",
      "step: 27801, loss: 6.667508125305176\n",
      "step: 27901, loss: 6.659797668457031\n",
      "step: 28001, loss: 6.613840103149414\n",
      "step: 28101, loss: 6.710870742797852\n",
      "step: 28201, loss: 6.603707790374756\n",
      "step: 28301, loss: 6.679915428161621\n",
      "step: 28401, loss: 6.609201431274414\n",
      "step: 28501, loss: 6.616242408752441\n",
      "step: 28601, loss: 6.638587951660156\n",
      "step: 28701, loss: 6.631647109985352\n",
      "step: 28801, loss: 6.624634265899658\n",
      "step: 28901, loss: 6.665033340454102\n",
      "step: 29001, loss: 6.6360273361206055\n",
      "step: 29101, loss: 6.665361404418945\n",
      "step: 29201, loss: 6.716558933258057\n",
      "step: 29301, loss: 6.698550224304199\n",
      "step: 29401, loss: 6.6713738441467285\n",
      "step: 29501, loss: 6.666018486022949\n",
      "step: 29601, loss: 6.68361234664917\n",
      "step: 29701, loss: 6.6856255531311035\n",
      "step: 29801, loss: 6.628157138824463\n",
      "step: 29901, loss: 6.661174774169922\n",
      "true_loss:  6.6315913\n",
      "loss:  6.712646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/tensorflow/lib/python3.5/site-packages/matplotlib/pyplot.py:513: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/home/pawel/tensorflow/lib/python3.5/site-packages/ipywidgets/widgets/widget.py:411: DeprecationWarning: Passing unrecoginized arguments to super(FigureCanvasNbAgg).__init__().\n",
      "__init__() missing 1 required positional argument: 'figure'\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "  super(Widget, self).__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473cdba710c94c82912e13b0bc34d39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model pdf integral: 0.976090\n",
      "true pdf integral: 0.979348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/tensorflow/lib/python3.5/site-packages/matplotlib/pyplot.py:513: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/home/pawel/tensorflow/lib/python3.5/site-packages/ipywidgets/widgets/widget.py:411: DeprecationWarning: Passing unrecoginized arguments to super(FigureCanvasNbAgg).__init__().\n",
      "__init__() missing 1 required positional argument: 'figure'\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "  super(Widget, self).__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3042e819a1aa4c7782f01e8b59dbcb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/tensorflow/lib/python3.5/site-packages/matplotlib/pyplot.py:513: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/home/pawel/tensorflow/lib/python3.5/site-packages/ipywidgets/widgets/widget.py:411: DeprecationWarning: Passing unrecoginized arguments to super(FigureCanvasNbAgg).__init__().\n",
      "__init__() missing 1 required positional argument: 'figure'\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "  super(Widget, self).__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e841500c07fb460ba9f57f139327d833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/tensorflow/lib/python3.5/site-packages/matplotlib/pyplot.py:513: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/home/pawel/tensorflow/lib/python3.5/site-packages/ipywidgets/widgets/widget.py:411: DeprecationWarning: Passing unrecoginized arguments to super(FigureCanvasNbAgg).__init__().\n",
      "__init__() missing 1 required positional argument: 'figure'\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "  super(Widget, self).__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89fe34d14aa4680a51e0124fccaab5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/tensorflow/lib/python3.5/site-packages/matplotlib/pyplot.py:513: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/home/pawel/tensorflow/lib/python3.5/site-packages/ipywidgets/widgets/widget.py:411: DeprecationWarning: Passing unrecoginized arguments to super(FigureCanvasNbAgg).__init__().\n",
      "__init__() missing 1 required positional argument: 'figure'\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "  super(Widget, self).__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cc5180e9e44028b8aed31a34fa3afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(10)\n",
    "global_step_tensor = tf.Variable(0, trainable=False, name='global_step')\n",
    "corr_coef = 0.7\n",
    "corr_ = np.array([[1.0,corr_coef],[corr_coef,1.0]], dtype=np.float32)\n",
    "std_dev = np.array([[7.0,0.0],[0,9.0]], dtype=np.float32)\n",
    "true_cov = np.matmul(std_dev, np.matmul(corr_,std_dev))\n",
    "print(\"true_cov: %s\"%true_cov)\n",
    "\n",
    "y_size = 2\n",
    "samples = 1000\n",
    "\n",
    "if y_size == 2:\n",
    "    data_gen = tp.distributions.MultivariateNormalFullCovariance(loc=tf.constant([0.0] * 2),\n",
    "                                                                 covariance_matrix=tf.constant(true_cov))\n",
    "    sample_op = data_gen.sample([samples])\n",
    "elif y_size==1:\n",
    "    data_gen = tp.distributions.Normal(loc=tf.constant(0.0), scale=3.0)\n",
    "    sample_op = tf.reshape(data_gen.sample([samples]), [-1,1])\n",
    "else:\n",
    "    raise ValueError(\"wrong size\")\n",
    "    \n",
    "\n",
    "dataset_gen = tf.data.Dataset.from_tensors(sample_op)\n",
    "y_ph=tf.placeholder(dtype=tf.float32, shape=[None,2])\n",
    "dataset_ph = tf.data.Dataset.from_tensors(y_ph)\n",
    "\n",
    "iter = tf.data.Iterator.from_structure(dataset_gen.output_types,output_shapes=(tf.TensorShape([None,2])))\n",
    "\n",
    "y=iter.get_next()\n",
    "y=tf.Print(y,[tf.train.get_global_step(),y], message=\"==========================================================start: \", summarize=1)\n",
    "x_size, x= 1 , tf.ones(shape=(tf.shape(y)[0],1), dtype=tf.float32)\n",
    "\n",
    "gen_init_op = iter.make_initializer(dataset_gen)\n",
    "ph_init_op = iter.make_initializer(dataset_ph)\n",
    "\n",
    "mode=tf.estimator.ModeKeys.TRAIN\n",
    "params={}\n",
    "params['positive_transform'] = \"square\"\n",
    "params['learning_rate'] = 0.0001\n",
    "params['arch1'] = [1]\n",
    "params['arch2'] = [10]\n",
    "params['arch3'] = [10]\n",
    "# params['relu_bias_initializer']=100.0\n",
    "\n",
    "positive_transform = params['positive_transform']\n",
    "learning_rate = params[\"learning_rate\"]\n",
    "\n",
    "if y is None or y.shape[-1].value != 2:\n",
    "    raise NotImplementedError\n",
    "\n",
    "y_components = [tf.slice(y, [0,i],[-1,1]) for i in range(y_size)]\n",
    "y_components_combined = tf.concat(y_components, axis=1)\n",
    "\n",
    "\n",
    "xy_prefinal,cdf_non_normalized, cdf_normalization, y_min,y_max,cdf = cdf_transform_normalizd(x, y_components_combined, params, positive_transform, mode)\n",
    "\n",
    "grads = create_pdf_layer_mv(cdf, y_components)\n",
    "\n",
    "pdf = tf.maximum(grads,1e-24) \n",
    "log_likelihood = tf.log(pdf, name=\"log_likelihood\")\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(log_likelihood), name=\"loss\")\n",
    "\n",
    "dlosss_dpdf = tf.gradients(loss,grads)[0]\n",
    "\n",
    "loss = tf.Print(loss,[tf.train.get_global_step(),loss], message=\"loss: \", summarize=1000)\n",
    "true_loss = tf.negative(tf.reduce_mean(data_gen.log_prob(y)), name=\"loss_true\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "# optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "\n",
    "train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "# with tf.name_scope(\"train_op\"):\n",
    "#     trainables = tf.trainable_variables()\n",
    "\n",
    "#     grads = tf.gradients(loss, trainables)\n",
    "\n",
    "# #     grads, _ = tf.clip_by_global_norm(grads, clip_norm=1)\n",
    "# #     grads, _ = tf.clip_by_value(grads, clip_value_max=1, clip_value_min=-1)\n",
    "\n",
    "#     grads_printed = []\n",
    "#     grads_clipped = []\n",
    "#     for grad, var in zip(grads, trainables):\n",
    "# #         print(var.name, var, grad.name, grad)\n",
    "# #         grads_printed.append(tf.Print(grad,[tf.train.get_global_step() ,grad], message=\"grad_%s: \"%var.name, summarize=100))\n",
    "#         grads_clipped.append(tf.clip_by_value(grad, -100, 100))\n",
    "# #         grads_clipped.append(tf.clip_by_norm(grad, 0.01))\n",
    "    \n",
    "# #     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  \n",
    "#     train_op = optimizer.apply_gradients(zip(grads_clipped, trainables), global_step=tf.train.get_global_step())\n",
    "\n",
    "delta=30\n",
    "y_centre_1 = 0\n",
    "y_centre_2 = 0\n",
    "points=100\n",
    "y_1 = np.linspace(y_centre_1-delta,y_centre_1+delta,points)\n",
    "y_2 = np.linspace(y_centre_2-delta,y_centre_2+delta,points)\n",
    "\n",
    "y_grid = np.meshgrid(y_1,y_2)\n",
    "\n",
    "y_grid_flat = np.concatenate(list(map(lambda a: a.reshape(-1,1), y_grid)), axis=1)\n",
    "\n",
    "def show_cdf():\n",
    "    session.run(ph_init_op, feed_dict={y_ph:y_grid_flat})\n",
    "    cdf_vals,pdf_vals = session.run([cdf,pdf])\n",
    "    true_cdf_vals=multivariate_normal.cdf(y_grid_flat, cov=true_cov)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_wireframe(y_grid[0], y_grid[1],cdf_vals.reshape(points,points),cmap=cm.coolwarm)\n",
    "    ax.plot_wireframe(y_grid[0], y_grid[1],true_cdf_vals.reshape(points,points),cmap=cm.gray)\n",
    "    plt.title(\"cdf\")\n",
    "    plt.xlabel(\"y1\")\n",
    "    plt.ylabel(\"y2\")\n",
    "    plt.show() \n",
    "    \n",
    "#     fig = plt.figure(figsize=(10,6))\n",
    "#     ax = fig.gca(projection='3d')\n",
    "#     ax.set_xlabel(\"y1\");\n",
    "#     ax.set_ylabel(\"y2\");\n",
    "#     ax.set_zlabel(\"cdf\");\n",
    "    \n",
    "#     surf=ax.plot_surface(y_grid[0],y_grid[1],cdf_vals.reshape(points,points),cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "    \n",
    "    \n",
    "#     ax.plot_surface(y_grid[0],y_grid[1],true_cdf_vals.reshape(points,points),cmap=cm.gray, linewidth=0, antialiased=False)\n",
    "    \n",
    "#     ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "#     ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "#     # Add a color bar which maps values to colors.\n",
    "#     fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    \n",
    "#     plt.show()\n",
    "    \n",
    "    true_pdf_vals=multivariate_normal.pdf(y_grid_flat, cov=true_cov)\n",
    "    \n",
    "    print(\"model pdf integral: %f\"%(((2*delta)**2)*np.sum(pdf_vals)/len(pdf_vals)))\n",
    "    print(\"true pdf integral: %f\"%(((2*delta)**2)*np.sum(true_pdf_vals)/len(true_pdf_vals)))\n",
    "#     print(\"pdf_vals: %r\"%pdf_vals[:10])\n",
    "#     print(\"true_pdf_vals: %r\"%true_pdf_vals[:10])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_wireframe(y_grid[0], y_grid[1],pdf_vals.reshape(points,points))\n",
    "    plt.title(\"model pdf\")\n",
    "    plt.xlabel(\"y1\")\n",
    "    plt.ylabel(\"y2\")\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    extent = [y_1[0], y_1[-1], y_2[0], y_2[-1]]\n",
    "    plt.ylabel('y2')\n",
    "    plt.xlabel('y1')\n",
    "    im=plt.imshow(pdf_vals.reshape(points,points), extent=extent)\n",
    "    plt.title(\"model pdf\")\n",
    "    plt.colorbar(im);\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_wireframe(y_grid[0], y_grid[1],true_pdf_vals.reshape(points,points))\n",
    "    plt.title(\"true pdf\")\n",
    "    plt.xlabel(\"y1\")\n",
    "    plt.ylabel(\"y2\")\n",
    "    plt.show() \n",
    "    \n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    extent = [y_1[0], y_1[-1], y_2[0], y_2[-1]]\n",
    "    plt.ylabel('y2')\n",
    "    plt.xlabel('y1')\n",
    "    plt.title(\"true pdf\")\n",
    "    im=plt.imshow(true_pdf_vals.reshape(points,points), extent=extent)\n",
    "    plt.colorbar(im);\n",
    "    plt.show()    \n",
    "    \n",
    "\n",
    "def train_loop(iters, tain_op):\n",
    "    session.run(gen_init_op)\n",
    "    print(\"step, loss and true_loss: \",session.run([tf.train.get_global_step(),loss,true_loss]))\n",
    "  \n",
    "    for step in range(iters):  \n",
    "        session.run(gen_init_op)\n",
    "        step_val,loss_val,_=session.run([tf.train.get_global_step(),loss,tain_op])\n",
    "        if step % 100 ==0:\n",
    "            print(\"step: {step}, loss: {loss}\".format(step=step_val,loss=loss_val))\n",
    "\n",
    "\n",
    "#                 variables_names = [v.name for v in tf.trainable_variables()]\n",
    "#                 values = session.run(variables_names)\n",
    "#                 for k, v in zip(variables_names, values):\n",
    "#                     file.write(\"Variable: {var} , shape: {shape}, val: {val}\\n\".format(var=k,shape=v.shape,val=v))\n",
    "\n",
    "#         file.write(\"min cdf: %.10f\\n\"%min_cdf_val)\n",
    "#         file.write(\"max cdf: %.10f\\n\"%max_cdf_val)\n",
    "#         file.write(\"min pdf: %.10f\\n\"%min_pdf_val)\n",
    "#         file.write(\"max pdf: %.10f\\n\"%max_pdf_val)\n",
    "#             print(\"cdf_non_normalized_vals \",cdf_non_normalized_vals)\n",
    "#             print(\"cdf_normalization_vals \",cdf_normalization_vals)\n",
    "#             print(\"xy_prefinal: \", xy_prefinal_vals)\n",
    "#         file.write(\"y min: %r\\n\"%y_min_vals)\n",
    "#         file.write(\"y max: %r\\n\"% y_max_vals)\n",
    "\n",
    "#         session.run(ph_init_op, feed_dict={y_ph:y_min_vals})\n",
    "#         file.write(\"CDF at min: %f\\n\"%session.run(cdf)) \n",
    "\n",
    "#         session.run(ph_init_op, feed_dict={y_ph:y_max_vals})\n",
    "#         file.write(\"CDF at max: %f\\n\"%session.run(cdf)) \n",
    "\n",
    "#                 file.write(\"grads_vals: %r\\n\"%grads_vals) \n",
    "\n",
    "#         file.flush()\n",
    "#         os.fsync(file)\n",
    "\n",
    "    #             y_range= np.c_[np.linspace(y_min_vals[0][0],y_max_vals[0][0],10).reshape(-1,1),np.linspace(y_min_vals[0][1],y_max_vals[0][1],10).reshape(-1,1)]\n",
    "    #             session.run(ph_init_op, feed_dict={y_ph:y_range})\n",
    "    #             print(\"y range: \",y_range) \n",
    "    #             print(\"CDF range: \",session.run(cdf)) \n",
    "\n",
    "    #             y1_test = np.sort(np.random.uniform(-10,10,100))\n",
    "    #             y2_test = np.sort(np.random.uniform(-10,10,100))\n",
    "\n",
    "    #             session.run(ph_init_op, feed_dict={y_ph:np.c_[y1_test,y2_test]})\n",
    "    #             print(\"cdf_increasing: \", np.all(np.diff(session.run(cdf)) >= 0) ) \n",
    "\n",
    "    #             y2_test = np.tile(np.random.uniform(-10,10,1), 100) \n",
    "    #             y1_test = np.sort(np.random.uniform(-10,10,100))\n",
    "    #             session.run(ph_init_op, feed_dict={y_ph:np.c_[y1_test,y2_test]})\n",
    "    #             print(\"cdf1_increasing: \", np.all(np.diff(session.run(cdf)) >= 0) ) \n",
    "\n",
    "    #             y1_test = np.tile(np.random.uniform(-10,10,1), 100) \n",
    "    #             y2_test = np.sort(np.random.uniform(-10,10,100))\n",
    "    #             session.run(ph_init_op, feed_dict={y_ph:np.c_[y1_test,y2_test]})\n",
    "    #             print(\"cdf2_increasing: \", np.all(np.diff(session.run(cdf)) >= 0) ) \n",
    "        \n",
    "            \n",
    "    session.run(gen_init_op)\n",
    "    print(\"true_loss: \",session.run(true_loss))\n",
    "    session.run(gen_init_op)\n",
    "    print(\"loss: \",session.run(loss))\n",
    "    show_cdf()\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5))) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    train_loop(30000, train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-55.262043"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1e+24"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tf.constant(1e-24)\n",
    "l=tf.log(x)\n",
    "g= tf.gradients(l,x)[0]\n",
    "# z= tf.gradients(g,x)[0]\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5))) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(l)\n",
    "    session.run(g)\n",
    "#     session.run(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 2.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "a=tf.get_variable(initializer=1.0,name=\"a\")\n",
    "b=tf.assign(a,2.0)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5))) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run([a,b])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
